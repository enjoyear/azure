this = {HadoopRDD@7831} "abfss://customer1@adl0enterprise.dfs.core.windows.net/data/gutenberg/davinci.txt HadoopRDD[4] at textFile at SparkWordCount2.scala:80"
conf = {Configuration@7854} "Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml, __spark_hadoop_conf__.xml, file:/etc/spark2/2.6.5.3008-11/0/hive-site.xml"
 quietmode = true
 restrictSystemProps = false
 allowNullValueProperties = false
 resources = {ArrayList@7866}  size = 2
 finalParameters = {Collections$SetFromMap@7867}  size = 8
 loadDefaults = true
 updatingResource = {ConcurrentHashMap@7868}  size = 1010
 properties = {Properties@7869}  size = 1009
  0 = {Hashtable$Entry@7876} "dfs.journalnode.rpc-address" -> "0.0.0.0:8485"
  1 = {Hashtable$Entry@7877} "yarn.ipc.rpc.class" -> "org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC"
  2 = {Hashtable$Entry@7878} "mapreduce.job.maxtaskfailures.per.tracker" -> "3"
  3 = {Hashtable$Entry@7879} "yarn.client.max-cached-nodemanagers-proxies" -> "0"
  4 = {Hashtable$Entry@7880} "mapreduce.job.speculative.retry-after-speculate" -> "15000"
  5 = {Hashtable$Entry@7881} "ha.health-monitor.connect-retry-interval.ms" -> "1000"
  6 = {Hashtable$Entry@7882} "yarn.resourcemanager.work-preserving-recovery.enabled" -> "true"
  7 = {Hashtable$Entry@7883} "yarn.resourcemanager.monitor.capacity.preemption.total_preemption_per_round" -> "0.1"
  8 = {Hashtable$Entry@7884} "dfs.client.mmap.cache.size" -> "256"
  9 = {Hashtable$Entry@7885} "dfs.namenode.read-lock-reporting-threshold-ms" -> "5000"
  10 = {Hashtable$Entry@7886} "mapreduce.reduce.markreset.buffer.percent" -> "0.0"
  11 = {Hashtable$Entry@7887} "dfs.datanode.data.dir" -> "/mnt/resource/hadoop/hdfs/data"
  12 = {Hashtable$Entry@7888} "mapreduce.jobhistory.max-age-ms" -> "604800000"
  13 = {Hashtable$Entry@7889} "dfs.namenode.lazypersist.file.scrub.interval.sec" -> "300"
  14 = {Hashtable$Entry@8007} "mapreduce.job.ubertask.enable" -> "false"
  15 = {Hashtable$Entry@8010} "dfs.namenode.delegation.token.renew-interval" -> "86400000"
  16 = {Hashtable$Entry@8013} "yarn.nodemanager.log-aggregation.compression-type" -> "gz"
  17 = {Hashtable$Entry@8016} "dfs.namenode.replication.considerLoad" -> "true"
  18 = {Hashtable$Entry@8019} "mapreduce.job.complete.cancel.delegation.tokens" -> "true"
  19 = {Hashtable$Entry@8021} "mapreduce.jobhistory.datestring.cache.size" -> "200000"
  20 = {Hashtable$Entry@8024} "hadoop.security.kms.client.authentication.retry-count" -> "1"
  21 = {Hashtable$Entry@8027} "hadoop.ssl.enabled.protocols" -> "TLSv1,SSLv2Hello,TLSv1.1,TLSv1.2"
  22 = {Hashtable$Entry@8030} "dfs.namenode.retrycache.heap.percent" -> "0.03f"
  23 = {Hashtable$Entry@8033} "dfs.namenode.top.window.num.buckets" -> "10"
  24 = {Hashtable$Entry@8036} "yarn.resourcemanager.scheduler.address" -> "hn0-chen-a.5ezjvsrqexeutmhucloqhdd3le.bx.internal.cloudapp.net:8030"
  25 = {Hashtable$Entry@8039} "hadoop.http.cross-origin.enabled" -> "false"
  26 = {Hashtable$Entry@8041} "dfs.client.file-block-storage-locations.num-threads" -> "10"
  27 = {Hashtable$Entry@8043} "dfs.datanode.balance.bandwidthPerSec" -> "6250000"
  28 = {Hashtable$Entry@8046} "yarn.resourcemanager.proxy-user-privileges.enabled" -> "false"
  29 = {Hashtable$Entry@8048} "dfs.namenode.decommission.max.concurrent.tracked.nodes" -> "100"
  30 = {Hashtable$Entry@8051} "mapreduce.reduce.shuffle.fetch.retry.enabled" -> "1"
  31 = {Hashtable$Entry@8053} "fs.azure.account.oauth2.client.secret" -> "modified->eVtWi8nUKvuCj+=/9k=X"
  32 = {Hashtable$Entry@8056} "yarn.nodemanager.resourcemanager.minimum.version" -> "NONE"
  33 = {Hashtable$Entry@8059} "io.mapfile.bloom.error.rate" -> "0.005"
  34 = {Hashtable$Entry@8062} "yarn.resourcemanager.nodemanagers.heartbeat-interval-ms" -> "1000"
  35 = {Hashtable$Entry@8065} "fs.azure.user.agent.prefix" -> "User-Agent: APN/1.0 Hortonworks/1.0 HDP/2.6.5.3008-11"
  36 = {Hashtable$Entry@8068} "dfs.secondary.namenode.kerberos.internal.spnego.principal" -> "${dfs.web.authentication.kerberos.principal}"
  37 = {Hashtable$Entry@8071} "hadoop.http.cross-origin.allowed-headers" -> "X-Requested-With,Content-Type,Accept,Origin"
  38 = {Hashtable$Entry@8074} "yarn.nodemanager.delete.debug-delay-sec" -> "600"
  39 = {Hashtable$Entry@8077} "dfs.namenode.write-lock-reporting-threshold-ms" -> "1000"
  40 = {Hashtable$Entry@8079} "dfs.client.read.shortcircuit.streams.cache.size" -> "4096"
  41 = {Hashtable$Entry@8082} "dfs.image.transfer.bandwidthPerSec" -> "0"
  42 = {Hashtable$Entry@8085} "yarn.scheduler.maximum-allocation-vcores" -> "7"
  43 = {Hashtable$Entry@8088} "yarn.resourcemanager.webapp.rest-csrf.enabled" -> "false"
  44 = {Hashtable$Entry@8090} "yarn.timeline-service.address" -> "headnodehost:10200"
  45 = {Hashtable$Entry@8093} "yarn.webapp.xfs-filter.enabled" -> "true"
  46 = {Hashtable$Entry@8095} "yarn.nodemanager.disk-health-checker.min-free-space-per-disk-mb" -> "1000"
  47 = {Hashtable$Entry@8097} "mapreduce.job.hdfs-servers" -> "${fs.defaultFS}"
  48 = {Hashtable$Entry@8100} "fs.wasb.impl" -> "org.apache.hadoop.fs.azure.NativeAzureFileSystem"
  49 = {Hashtable$Entry@8103} "mapreduce.task.profile.reduce.params" -> "${mapreduce.task.profile.params}"
  50 = {Hashtable$Entry@8106} "dfs.namenode.fs-limits.min-block-size" -> "1048576"
  51 = {Hashtable$Entry@8109} "ftp.stream-buffer-size" -> "4096"
  52 = {Hashtable$Entry@8111} "dfs.client.use.legacy.blockreader.local" -> "false"
  53 = {Hashtable$Entry@8113} "hadoop.http.cross-origin.allowed-methods" -> "GET,POST,HEAD"
  54 = {Hashtable$Entry@8116} "dfs.short.circuit.shared.memory.watcher.interrupt.check.ms" -> "60000"
  55 = {Hashtable$Entry@8119} "dfs.datanode.directoryscan.threads" -> "1"
  56 = {Hashtable$Entry@8121} "fs.s3a.buffer.dir" -> "${hadoop.tmp.dir}/s3a"
  57 = {Hashtable$Entry@8124} "yarn.client.application-client-protocol.poll-interval-ms" -> "200"
  58 = {Hashtable$Entry@8127} "yarn.timeline-service.leveldb-timeline-store.path" -> "/hadoop/yarn/timeline"
  59 = {Hashtable$Entry@8130} "mapreduce.job.split.metainfo.maxsize" -> "10000000"
  60 = {Hashtable$Entry@8133} "dfs.namenode.edits.noeditlogchannelflush" -> "false"
  61 = {Hashtable$Entry@8135} "fs.s3a.fast.upload.buffer" -> "disk"
  62 = {Hashtable$Entry@8138} "s3native.bytes-per-checksum" -> "512"
  63 = {Hashtable$Entry@8141} "yarn.client.failover-retries-on-socket-timeouts" -> "0"
  64 = {Hashtable$Entry@8143} "hadoop.security.sensitive-config-keys" -> "\n      secret$\n      password$\n      ssl.keystore.pass$\n      fs.s3a.server-side-encryption.key\n      fs.s3a.*.server-side-encryption.key\n      fs.s3a.secret.key\n      fs.s3a.*.secret.key\n      fs.s3a.session.key\n      fs.s3a.*.session.key\n      fs.s3a.session.token\n      fs.s3a.*.session.token\n      fs.azure.account.key.*\n      fs.azure.oauth2.*\n      fs.adl.oauth2.*\n      credential$\n      oauth.*secret\n      oauth.*password\n      oauth.*token\n      hadoop.security.sensitive-config-keys\n      fs.s3.*[Ss]ecret.?[Kk]ey\n  "
  65 = {Hashtable$Entry@8146} "dfs.namenode.startup.delay.block.deletion.sec" -> "3600"
  66 = {Hashtable$Entry@8149} "dfs.webhdfs.user.provider.user.pattern" -> "^[A-Za-z_][A-Za-z0-9._-]*[$]?$"
  67 = {Hashtable$Entry@8152} "cloneConf" -> "true"
  68 = {Hashtable$Entry@8155} "yarn.nodemanager.webapp.rest-csrf.custom-header" -> "X-XSRF-Header"
  69 = {Hashtable$Entry@8158} "mapreduce.tasktracker.tasks.sleeptimebeforesigkill" -> "5000"
  70 = {Hashtable$Entry@8161} "yarn.timeline-service.client.retry-interval-ms" -> "1000"
  71 = {Hashtable$Entry@8163} "dfs.encrypt.data.transfer.cipher.key.bitlength" -> "128"
  72 = {Hashtable$Entry@8166} "yarn.timeline-service.entity-group-fs-store.with-user-dir" -> "false"
  73 = {Hashtable$Entry@8168} "hive.server2.enable.doAs" -> "false"
  74 = {Hashtable$Entry@8170} "fs.AbstractFileSystem.wasbs.impl" -> "org.apache.hadoop.fs.azure.Wasbs"
  75 = {Hashtable$Entry@8173} "hadoop.http.authentication.type" -> "simple"
  76 = {Hashtable$Entry@8176} "dfs.namenode.rpc-address.mycluster.nn2" -> "hn1-chen-a.5ezjvsrqexeutmhucloqhdd3le.bx.internal.cloudapp.net:8020"
  77 = {Hashtable$Entry@8179} "dfs.namenode.path.based.cache.refresh.interval.ms" -> "30000"
  78 = {Hashtable$Entry@8182} "dfs.namenode.rpc-address.mycluster.nn1" -> "hn0-chen-a.5ezjvsrqexeutmhucloqhdd3le.bx.internal.cloudapp.net:8020"
  79 = {Hashtable$Entry@8185} "yarn.nodemanager.linux-container-executor.cgroups.mount-path" -> "/cgroup"
  80 = {Hashtable$Entry@8188} "mapreduce.local.clientfactory.class.name" -> "org.apache.hadoop.mapred.LocalClientFactory"
  81 = {Hashtable$Entry@8191} "dfs.namenode.max.full.block.report.leases" -> "6"
  82 = {Hashtable$Entry@8194} "dfs.datanode.cache.revocation.timeout.ms" -> "900000"
  83 = {Hashtable$Entry@8197} "ipc.client.connection.maxidletime" -> "30000"
  84 = {Hashtable$Entry@8199} "ipc.server.max.connections" -> "0"
  85 = {Hashtable$Entry@8202} "mapreduce.jobhistory.recovery.store.leveldb.path" -> "/hadoop/mapreduce/jhs"
  86 = {Hashtable$Entry@8205} "dfs.namenode.safemode.threshold-pct" -> "0.99f"
  87 = {Hashtable$Entry@8208} "fs.s3a.multipart.purge.age" -> "86400"
  88 = {Hashtable$Entry@8211} "dfs.namenode.num.checkpoints.retained" -> "2"
  89 = {Hashtable$Entry@8214} "yarn.timeline-service.client.best-effort" -> "true"
  90 = {Hashtable$Entry@8217} "mapreduce.jobhistory.webapp.xfs-filter.xframe-options" -> "SAMEORIGIN"
  91 = {Hashtable$Entry@8220} "fs.azure.authorization" -> "false"
  92 = {Hashtable$Entry@8223} "yarn.timeline-service.bind-host" -> "0.0.0.0"
  93 = {Hashtable$Entry@8226} "mapreduce.job.ubertask.maxmaps" -> "9"
  94 = {Hashtable$Entry@8229} "dfs.namenode.stale.datanode.interval" -> "30000"
  95 = {Hashtable$Entry@8231} "dfs.client.failover.proxy.provider.mycluster" -> "org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider"
  96 = {Hashtable$Entry@8234} "yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage" -> "90"
  97 = {Hashtable$Entry@8237} "mapreduce.tasktracker.http.address" -> "0.0.0.0:50060"
  98 = {Hashtable$Entry@8240} "mapreduce.ifile.readahead.bytes" -> "4194304"
  99 = {Hashtable$Entry@8243} "mapreduce.jobhistory.webapp.rest-csrf.enabled" -> "false"
  100 = {Hashtable$Entry@8249} "yarn.sharedcache.uploader.server.thread-count" -> "50"
  101 = {Hashtable$Entry@8250} "mapreduce.jobhistory.admin.address" -> "headnodehost:10033"
  102 = {Hashtable$Entry@8251} "s3.client-write-packet-size" -> "65536"
  103 = {Hashtable$Entry@8252} "dfs.block.access.token.lifetime" -> "600"
  104 = {Hashtable$Entry@8253} "yarn.app.mapreduce.am.resource.cpu-vcores" -> "1"
  105 = {Hashtable$Entry@8254} "mapreduce.input.lineinputformat.linespermap" -> "1"
  106 = {Hashtable$Entry@8255} "hive.metastore.client.connect.retry.delay" -> "5"
  107 = {Hashtable$Entry@8256} "dfs.namenode.num.extra.edits.retained" -> "1000000"
  108 = {Hashtable$Entry@8257} "mapreduce.reduce.shuffle.input.buffer.percent" -> "0.7"
  109 = {Hashtable$Entry@8258} "hadoop.http.staticuser.user" -> "dr.who"
  110 = {Hashtable$Entry@8259} "mapreduce.reduce.maxattempts" -> "10"
  111 = {Hashtable$Entry@8260} "hadoop.security.group.mapping.ldap.search.filter.user" -> "(&(objectClass=user)(sAMAccountName={0}))"
  112 = {Hashtable$Entry@8261} "mapreduce.jobhistory.admin.acl" -> "*"
  113 = {Hashtable$Entry@8262} "hadoop.workaround.non.threadsafe.getpwuid" -> "false"
  114 = {Hashtable$Entry@8263} "dfs.client.context" -> "default"
  115 = {Hashtable$Entry@8264} "mapreduce.map.maxattempts" -> "10"
  116 = {Hashtable$Entry@8265} "yarn.timeline-service.entity-group-fs-store.active-dir" -> "/atshistory/active"
  117 = {Hashtable$Entry@8266} "yarn.resourcemanager.zk-retry-interval-ms" -> "1000"
  118 = {Hashtable$Entry@8267} "mapreduce.jobhistory.cleaner.interval-ms" -> "86400000"
  119 = {Hashtable$Entry@8268} "dfs.datanode.drop.cache.behind.reads" -> "false"
  120 = {Hashtable$Entry@8269} "dfs.permissions.superusergroup" -> "hdfs"
  121 = {Hashtable$Entry@8270} "yarn.application.classpath" -> "$HADOOP_CONF_DIR,/usr/hdp/current/hadoop-client/*,/usr/hdp/current/hadoop-client/lib/*,/usr/hdp/current/hadoop-hdfs-client/*,/usr/hdp/current/hadoop-hdfs-client/lib/*,/usr/hdp/current/hadoop-yarn-client/*,/usr/hdp/current/hadoop-yarn-client/lib/*"
  122 = {Hashtable$Entry@8271} "mapreduce.jobhistory.bind-host" -> "0.0.0.0"
  123 = {Hashtable$Entry@8272} "fs.s3n.block.size" -> "67108864"
  124 = {Hashtable$Entry@8273} "dfs.namenode.https-address.mycluster.nn2" -> "hn1-chen-a.5ezjvsrqexeutmhucloqhdd3le.bx.internal.cloudapp.net:30470"
  125 = {Hashtable$Entry@8274} "dfs.namenode.https-address.mycluster.nn1" -> "hn0-chen-a.5ezjvsrqexeutmhucloqhdd3le.bx.internal.cloudapp.net:30470"
  126 = {Hashtable$Entry@8275} "hadoop.registry.system.acls" -> "sasl:yarn@, sasl:mapred@, sasl:hdfs@"
  127 = {Hashtable$Entry@8276} "dfs.namenode.list.cache.pools.num.responses" -> "100"
  128 = {Hashtable$Entry@8404} "yarn.nodemanager.kill-escape.user" -> "hive"
  129 = {Hashtable$Entry@8407} "yarn.resourcemanager.resource-tracker.address.rm2" -> "hn1-chen-a.5ezjvsrqexeutmhucloqhdd3le.bx.internal.cloudapp.net:8025"
  130 = {Hashtable$Entry@8410} "yarn.resourcemanager.resource-tracker.address.rm1" -> "hn0-chen-a.5ezjvsrqexeutmhucloqhdd3le.bx.internal.cloudapp.net:8025"
  131 = {Hashtable$Entry@8413} "dfs.datanode.slow.io.warning.threshold.ms" -> "300"
  132 = {Hashtable$Entry@8416} "yarn.sharedcache.store.in-memory.check-period-mins" -> "720"
  133 = {Hashtable$Entry@8419} "fs.s3a.multiobjectdelete.enable" -> "true"
  134 = {Hashtable$Entry@8422} "yarn.nodemanager.bind-host" -> "0.0.0.0"
  135 = {Hashtable$Entry@8425} "dfs.namenode.fs-limits.max-blocks-per-file" -> "1048576"
  136 = {Hashtable$Entry@8428} "yarn.nodemanager.vmem-check-enabled" -> "false"
  137 = {Hashtable$Entry@8431} "hadoop.security.authentication" -> "simple"
  138 = {Hashtable$Entry@8434} "mapreduce.reduce.cpu.vcores" -> "1"
  139 = {Hashtable$Entry@8437} "net.topology.node.switch.mapping.impl" -> "org.apache.hadoop.net.ScriptBasedMapping"
  140 = {Hashtable$Entry@8440} "fs.s3.sleepTimeSeconds" -> "10"
  141 = {Hashtable$Entry@8443} "dfs.datanode.peer.stats.enabled" -> "false"
  142 = {Hashtable$Entry@8445} "yarn.timeline-service.ttl-ms" -> "604800000"
  143 = {Hashtable$Entry@8448} "yarn.sharedcache.root-dir" -> "/sharedcache"
  144 = {Hashtable$Entry@8451} "yarn.resourcemanager.keytab" -> "/etc/krb5.keytab"
  145 = {Hashtable$Entry@8454} "yarn.resourcemanager.container.liveness-monitor.interval-ms" -> "600000"
  146 = {Hashtable$Entry@8457} "mapreduce.jobtracker.heartbeats.in.second" -> "100"
  147 = {Hashtable$Entry@8459} "yarn.node-labels.fs-store.root-dir" -> "/system/yarn/node-labels"
  148 = {Hashtable$Entry@8462} "hadoop.security.group.mapping.ldap.posix.attr.gid.name" -> "gidNumber"
  149 = {Hashtable$Entry@8465} "yarn.app.mapreduce.am.scheduler.heartbeat.interval-ms" -> "1000"
  150 = {Hashtable$Entry@8468} "yarn.app.mapreduce.client-am.ipc.max-retries-on-timeouts" -> "3"
  151 = {Hashtable$Entry@8471} "yarn.nodemanager.linux-container-executor.cgroups.hierarchy" -> "hadoop-yarn"
  152 = {Hashtable$Entry@8474} "yarn.resourcemanager.delegation-token.max-conf-size-bytes" -> "12800"
  153 = {Hashtable$Entry@8477} "s3.bytes-per-checksum" -> "512"
  154 = {Hashtable$Entry@8480} "hadoop.ssl.require.client.cert" -> "false"
  155 = {Hashtable$Entry@8482} "yarn.resourcemanager.ha.rm-ids" -> "rm1,rm2"
  156 = {Hashtable$Entry@8485} "dfs.journalnode.http-address" -> "0.0.0.0:8480"
  157 = {Hashtable$Entry@8488} "mapreduce.output.fileoutputformat.compress" -> "false"
  158 = {Hashtable$Entry@8490} "dfs.ha.automatic-failover.enabled" -> "true"
  159 = {Hashtable$Entry@8492} "dfs.namenode.metrics.logger.period.seconds" -> "600"
  160 = {Hashtable$Entry@8495} "yarn.resourcemanager.webapp.delegation-token-auth-filter.enabled" -> "false"
  161 = {Hashtable$Entry@8497} "mapreduce.shuffle.max.threads" -> "0"
  162 = {Hashtable$Entry@8500} "mapreduce.jobhistory.webapp.rest-csrf.custom-header" -> "X-XSRF-Header"
  163 = {Hashtable$Entry@8503} "dfs.namenode.invalidate.work.pct.per.iteration" -> "0.32f"
  164 = {Hashtable$Entry@8506} "s3native.client-write-packet-size" -> "65536"
  165 = {Hashtable$Entry@8509} "dfs.namenode.max-lock-hold-to-release-lease-ms" -> "25"
  166 = {Hashtable$Entry@8512} "dfs.client.block.write.replace-datanode-on-failure.policy" -> "DEFAULT"
  167 = {Hashtable$Entry@8515} "mapreduce.client.submit.file.replication" -> "10"
  168 = {Hashtable$Entry@8517} "yarn.app.mapreduce.am.job.committer.commit-window" -> "10000"
  169 = {Hashtable$Entry@8520} "dfs.namenode.audit.log.async" -> "true"
  170 = {Hashtable$Entry@8522} "yarn.nodemanager.sleep-delay-before-sigkill.ms" -> "250"
  171 = {Hashtable$Entry@8525} "yarn.nodemanager.env-whitelist" -> "JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,HADOOP_YARN_HOME,HADOOP_HOME,PATH,LANG,TZ"
  172 = {Hashtable$Entry@8528} "dfs.namenode.acls.enabled" -> "false"
  173 = {Hashtable$Entry@8530} "dfs.namenode.secondary.http-address" -> "localhost:50090"
  174 = {Hashtable$Entry@8533} "mapreduce.map.speculative" -> "false"
  175 = {Hashtable$Entry@8535} "yarn.nodemanager.linux-container-executor.cgroups.mount" -> "false"
  176 = {Hashtable$Entry@8537} "mapreduce.job.speculative.slowtaskthreshold" -> "1.0"
  177 = {Hashtable$Entry@8540} "mapreduce.tasktracker.http.threads" -> "40"
  178 = {Hashtable$Entry@8543} "mapreduce.jobhistory.http.policy" -> "HTTP_ONLY"
  179 = {Hashtable$Entry@8546} "mapreduce.jobtracker.webinterface.trusted" -> "false"
  180 = {Hashtable$Entry@8548} "fs.s3a.paging.maximum" -> "5000"
  181 = {Hashtable$Entry@8551} "hadoop.kerberos.min.seconds.before.relogin" -> "60"
  182 = {Hashtable$Entry@8554} "yarn.resourcemanager.nodemanager-connect-retries" -> "10"
  183 = {Hashtable$Entry@8556} "fs.s3.buffer.dir" -> "${hadoop.tmp.dir}/s3"
  184 = {Hashtable$Entry@8559} "hadoop.cache.data.fullness.percentage" -> "50"
  185 = {Hashtable$Entry@8562} "io.native.lib.available" -> "true"
  186 = {Hashtable$Entry@8564} "dfs.namenode.heartbeat.recheck-interval" -> "300000"
  187 = {Hashtable$Entry@8567} "fs.AbstractFileSystem.wasb.impl" -> "org.apache.hadoop.fs.azure.Wasb"
  188 = {Hashtable$Entry@8570} "fs.azure.account.key.abs0jar.blob.core.windows.net" -> "MIIB/modified->jCCAeoCAQAxggFdMIIBWQIBADBBMC0xKzApBgNVBAMTImRiZW5jcnlwdGlvbi5oZGluc2lnaHRzZXJ2aWNlcy5uZXQCEBSsRje2BBWjQSEZm1RPfpwwDQYJKoZIhvcNAQEBBQAEggEAQ83B2C9lg7aErMcc3FLhKTUfmTGQFXpx5LCYaBp3QKauBtErI88fD+LPOSZF2mjYwOjshPX6zgANxBsPambqQ8hwnMEpIaIA15Mvk2/lXLPs+c/TCIxttPMZoWdGQx4tptwuYrujfcwpaHQFjA62QrJU6OysR+/aNfmugmWpmqKXfBM/Xd8OZk77ABD5o9P1B5krkiApXN1T2sb1ASIcr5xEfVU3zodyP5c+IjW5bIkdg/K0KCw3H1KpYzXLPxpdc8OOxbj3YxK9NldLqOB1tlF/xtEaM3Sm4Qhokn3tI1KBD+wnTU+Wfasxrwz+b+7VDQa2w8Qoc7QXKRZT43KoiTCBgwYJKoZIhvcNAQcBMBQGCCqGSIb3DQMHBAg+gg94BMsdvIBg2NxwmdThNWz7KLvtZxkTPgGZB0wEhGh2WY+/EmB/8Hh3wDMPCJ+WdFgd5LWV3sizloGE2bIIbPAQ3IZ6RvB+xcn/sR0Iiq44P5OiXzA7wKlkwXZzunOQipyoRgsV0S33"
  189 = {Hashtable$Entry@8573} "mapreduce.jobhistory.done-dir" -> "/mr-history/done"
  190 = {Hashtable$Entry@8576} "hadoop.registry.zk.retry.interval.ms" -> "1000"
  191 = {Hashtable$Entry@8578} "mapreduce.job.reducer.unconditional-preempt.delay.sec" -> "300"
  192 = {Hashtable$Entry@8580} "dfs.namenode.avoid.write.stale.datanode" -> "true"
  193 = {Hashtable$Entry@8582} "dfs.namenode.checkpoint.txns" -> "1000000"
  194 = {Hashtable$Entry@8585} "hadoop.ssl.hostname.verifier" -> "DEFAULT"
  195 = {Hashtable$Entry@8587} "iocachedisabled.fs.AbstractFileSystem.wasb.impl" -> "org.apache.hadoop.fs.azure.Wasb"
  196 = {Hashtable$Entry@8589} "mapreduce.task.timeout" -> "300000"
  197 = {Hashtable$Entry@8591} "hadoop.service.shutdown.timeout" -> "30s"
  198 = {Hashtable$Entry@8594} "yarn.nodemanager.disk-health-checker.interval-ms" -> "120000"
  199 = {Hashtable$Entry@8597} "adl.feature.ownerandgroup.enableupn" -> "false"
  200 = {Hashtable$Entry@8602} "dfs.journalnode.https-address" -> "0.0.0.0:8481"
  201 = {Hashtable$Entry@8603} "hadoop.security.groups.cache.secs" -> "300"
  202 = {Hashtable$Entry@8604} "mapreduce.input.fileinputformat.split.minsize" -> "0"
  203 = {Hashtable$Entry@8605} "dfs.datanode.sync.behind.writes" -> "false"
  204 = {Hashtable$Entry@8606} "yarn.resourcemanager.fail-fast" -> "${yarn.fail-fast}"
  205 = {Hashtable$Entry@8607} "dfs.namenode.full.block.report.lease.length.ms" -> "300000"
  206 = {Hashtable$Entry@8608} "ipc.server.tcpnodelay" -> "true"
  207 = {Hashtable$Entry@8609} "mapreduce.shuffle.port" -> "13562"
  208 = {Hashtable$Entry@8610} "hadoop.rpc.protection" -> "authentication"
  209 = {Hashtable$Entry@8611} "dfs.client.https.keystore.resource" -> "ssl-client.xml"
  210 = {Hashtable$Entry@8612} "dfs.namenode.list.encryption.zones.num.responses" -> "100"
  211 = {Hashtable$Entry@8613} "yarn.timeline-service.recovery.enabled" -> "true"
  212 = {Hashtable$Entry@8614} "yarn.client.failover-proxy-provider" -> "org.apache.hadoop.yarn.client.RequestHedgingRMFailoverProxyProvider"
  213 = {Hashtable$Entry@8615} "mapreduce.jobtracker.retiredjobs.cache.size" -> "1000"
  214 = {Hashtable$Entry@8616} "dfs.ha.tail-edits.period" -> "60"
  215 = {Hashtable$Entry@8617} "dfs.datanode.drop.cache.behind.writes" -> "false"
  216 = {Hashtable$Entry@8618} "fs.s3.maxRetries" -> "4"
  217 = {Hashtable$Entry@8619} "mapreduce.jobtracker.address" -> "local"
  218 = {Hashtable$Entry@8620} "hadoop.http.authentication.kerberos.principal" -> "HTTP/_HOST@LOCALHOST"
  219 = {Hashtable$Entry@8621} "yarn.app.mapreduce.am.job.recovery.enable" -> "true"
  220 = {Hashtable$Entry@8622} "hadoop.security.group.mapping.ldap.posix.attr.uid.name" -> "uidNumber"
  221 = {Hashtable$Entry@8623} "nfs.server.port" -> "2049"
  222 = {Hashtable$Entry@8624} "yarn.resourcemanager.webapp.address" -> "hn0-chen-a.5ezjvsrqexeutmhucloqhdd3le.bx.internal.cloudapp.net:8088"
  223 = {Hashtable$Entry@8625} "dfs.qjm.operations.timeout" -> "5m"
  224 = {Hashtable$Entry@8626} "yarn.timeline-service.client.max-retries" -> "30"
  225 = {Hashtable$Entry@8627} "mapreduce.task.profile.reduces" -> "0-2"
  226 = {Hashtable$Entry@8628} "yarn.resourcemanager.am.max-attempts" -> "5"
  227 = {Hashtable$Entry@8629} "nfs.dump.dir" -> "/tmp/.hdfs-nfs"
  228 = {Hashtable$Entry@8630} "fs.azure.page.blob.dir" -> "/mapreducestaging,/atshistory,/tezstaging,/ams/hbase/WALs,/ams/hbase/oldWALs,/ams/hbase/MasterProcWALs"
  229 = {Hashtable$Entry@8631} "dfs.bytes-per-checksum" -> "512"
  230 = {Hashtable$Entry@8632} "mapreduce.job.end-notification.max.retry.interval" -> "5000"
  231 = {Hashtable$Entry@8633} "ipc.client.connect.retry.interval" -> "1000"
  232 = {Hashtable$Entry@8634} "fs.s3a.multipart.size" -> "67108864"
  233 = {Hashtable$Entry@8635} "fs.azure.account.keyprovider.abs0execution.blob.core.windows.net" -> "org.apache.hadoop.fs.azure.ShellDecryptionKeyProvider"
  234 = {Hashtable$Entry@8636} "yarn.app.mapreduce.am.command-opts" -> "-Xmx2560M -Xms2560M -Djava.net.preferIPv4Stack=true -XX:NewRatio=8 -XX:+UseNUMA -XX:+UseParallelGC"
  235 = {Hashtable$Entry@8637} "yarn.nodemanager.process-kill-wait.ms" -> "2000"
  236 = {Hashtable$Entry@8638} "yarn.timeline-service.state-store-class" -> "org.apache.hadoop.yarn.server.timeline.recovery.SqlDBTimelineStateStore"
  237 = {Hashtable$Entry@8639} "yarn.nodemanager.container.stderr.tail.bytes" -> "4096"
  238 = {Hashtable$Entry@8640} "dfs.namenode.safemode.min.datanodes" -> "0"
  239 = {Hashtable$Entry@8641} "yarn.timeline-service.client.fd-clean-interval-secs" -> "60"
  240 = {Hashtable$Entry@8642} "mapreduce.job.speculative.minimum-allowed-tasks" -> "10"
  241 = {Hashtable$Entry@8643} "dfs.namenode.write.stale.datanode.ratio" -> "1.0f"
  242 = {Hashtable$Entry@8644} "hadoop.jetty.logs.serve.aliases" -> "true"
  243 = {Hashtable$Entry@8645} "mapreduce.reduce.shuffle.fetch.retry.timeout-ms" -> "30000"
  244 = {Hashtable$Entry@8646} "fs.du.interval" -> "600000"
  245 = {Hashtable$Entry@8783} "mapreduce.tasktracker.dns.nameserver" -> "default"
  246 = {Hashtable$Entry@8786} "yarn.sharedcache.admin.address" -> "0.0.0.0:8047"
  247 = {Hashtable$Entry@8789} "mapreduce.admin.reduce.child.java.opts" -> "-server -XX:NewRatio=8 -Djava.net.preferIPv4Stack=true -Dhdp.version=${hdp.version}"
  248 = {Hashtable$Entry@8792} "hadoop.custom-extensions.root" -> "/hdp/ext/2.6/hadoop"
  249 = {Hashtable$Entry@8795} "hadoop.security.random.device.file.path" -> "/dev/urandom"
  250 = {Hashtable$Entry@8798} "mapreduce.task.merge.progress.records" -> "10000"
  251 = {Hashtable$Entry@8801} "dfs.webhdfs.enabled" -> "false"
  252 = {Hashtable$Entry@8804} "hadoop.registry.secure" -> "false"
  253 = {Hashtable$Entry@8806} "hadoop.ssl.client.conf" -> "ssl-client.xml"
  254 = {Hashtable$Entry@8809} "mapreduce.job.counters.max" -> "130"
  255 = {Hashtable$Entry@8812} "yarn.nodemanager.localizer.fetch.thread-count" -> "4"
  256 = {Hashtable$Entry@8815} "io.mapfile.bloom.size" -> "1048576"
  257 = {Hashtable$Entry@8818} "yarn.nodemanager.localizer.client.thread-count" -> "5"
  258 = {Hashtable$Entry@8821} "fs.automatic.close" -> "true"
  259 = {Hashtable$Entry@8824} "mapreduce.task.profile" -> "false"
  260 = {Hashtable$Entry@8826} "yarn.nodemanager.recovery.compaction-interval-secs" -> "3600"
  261 = {Hashtable$Entry@8829} "dfs.namenode.edit.log.autoroll.multiplier.threshold" -> "2.0"
  262 = {Hashtable$Entry@8832} "mapreduce.task.combine.progress.records" -> "10000"
  263 = {Hashtable$Entry@8834} "mapreduce.shuffle.ssl.file.buffer.size" -> "65536"
  264 = {Hashtable$Entry@8837} "yarn.app.mapreduce.client.job.max-retries" -> "30"
  265 = {Hashtable$Entry@8840} "fs.swift.impl" -> "org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystem"
  266 = {Hashtable$Entry@8843} "yarn.app.mapreduce.am.container.log.backups" -> "0"
  267 = {Hashtable$Entry@8846} "dfs.datanode.available-space-volume-choosing-policy.balanced-space-preference-fraction" -> "0.75f"
  268 = {Hashtable$Entry@8849} "dfs.namenode.backup.address" -> "0.0.0.0:50100"
  269 = {Hashtable$Entry@8852} "dfs.client.https.need-auth" -> "false"
  270 = {Hashtable$Entry@8854} "mapreduce.app-submission.cross-platform" -> "false"
  271 = {Hashtable$Entry@8856} "yarn.timeline-service.ttl-enable" -> "true"
  272 = {Hashtable$Entry@8858} "hadoop.security.group.mapping.ldap.conversion.rule" -> "none"
  273 = {Hashtable$Entry@8861} "dfs.user.home.dir.prefix" -> "/user"
  274 = {Hashtable$Entry@8864} "yarn.nodemanager.container-monitor.procfs-tree.smaps-based-rss.enabled" -> "false"
  275 = {Hashtable$Entry@8866} "yarn.nodemanager.keytab" -> "/etc/krb5.keytab"
  276 = {Hashtable$Entry@8869} "dfs.namenode.http-address.mycluster.nn2" -> "hn1-chen-a.5ezjvsrqexeutmhucloqhdd3le.bx.internal.cloudapp.net:30070"
  277 = {Hashtable$Entry@8872} "dfs.namenode.http-address.mycluster.nn1" -> "hn0-chen-a.5ezjvsrqexeutmhucloqhdd3le.bx.internal.cloudapp.net:30070"
  278 = {Hashtable$Entry@8875} "fs.azure.authorization.caching.enable" -> "true"
  279 = {Hashtable$Entry@8877} "yarn.app.mapreduce.am.create-intermediate-jh-base-dir" -> "true"
  280 = {Hashtable$Entry@8879} "dfs.namenode.xattrs.enabled" -> "true"
  281 = {Hashtable$Entry@8881} "yarn.app.mapreduce.am.admin-command-opts" -> "-Dhdp.version=${hdp.version}"
  282 = {Hashtable$Entry@8884} "nfs.file.dump.dir" -> "/tmp/.hdfs-nfs"
  283 = {Hashtable$Entry@8887} "dfs.client.write.exclude.nodes.cache.expiry.interval.millis" -> "600000"
  284 = {Hashtable$Entry@8889} "dfs.datanode.fileio.profiling.sampling.percentage" -> "0"
  285 = {Hashtable$Entry@8891} "yarn.sharedcache.client-server.address" -> "0.0.0.0:8045"
  286 = {Hashtable$Entry@8894} "mapreduce.jobtracker.restart.recover" -> "false"
  287 = {Hashtable$Entry@8896} "mapreduce.map.skip.proc.count.autoincr" -> "true"
  288 = {Hashtable$Entry@8898} "dfs.namenode.datanode.registration.ip-hostname-check" -> "false"
  289 = {Hashtable$Entry@8900} "dfs.image.transfer.chunksize" -> "65536"
  290 = {Hashtable$Entry@8902} "yarn.nodemanager.webapp.cross-origin.enabled" -> "false"
  291 = {Hashtable$Entry@8904} "yarn.nodemanager.runtime.linux.docker.privileged-containers.allowed" -> "false"
  292 = {Hashtable$Entry@8906} "hadoop.security.instrumentation.requires.admin" -> "false"
  293 = {Hashtable$Entry@8908} "io.compression.codec.bzip2.library" -> "system-native"
  294 = {Hashtable$Entry@8911} "yarn.nodemanager.webapp.rest-csrf.methods-to-ignore" -> "GET,OPTIONS,HEAD"
  295 = {Hashtable$Entry@8914} "dfs.namenode.name.dir.restore" -> "true"
  296 = {Hashtable$Entry@8916} "dfs.datanode.outliers.report.interval" -> "1800000"
  297 = {Hashtable$Entry@8919} "dfs.namenode.resource.checked.volumes.minimum" -> "1"
  298 = {Hashtable$Entry@8922} "hadoop.ssl.keystores.factory.class" -> "org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory"
  299 = {Hashtable$Entry@8925} "dfs.namenode.list.cache.directives.num.responses" -> "100"
  300 = {Hashtable$Entry@8929} "fs.ftp.host" -> "0.0.0.0"
  301 = {Hashtable$Entry@8930} "fs.wasbs.impl" -> "org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure"
  302 = {Hashtable$Entry@8931} "yarn.app.mapreduce.am.containerlauncher.threadpool-initial-size" -> "10"
  303 = {Hashtable$Entry@8932} "yarn.nodemanager.log-aggregation.debug-enabled" -> "false"
  304 = {Hashtable$Entry@8933} "s3.blocksize" -> "67108864"
  305 = {Hashtable$Entry@8934} "s3native.stream-buffer-size" -> "4096"
  306 = {Hashtable$Entry@8935} "fs.abfs.impl" -> "org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem"
  307 = {Hashtable$Entry@8936} "mapreduce.jobtracker.taskscheduler" -> "org.apache.hadoop.mapred.JobQueueTaskScheduler"
  308 = {Hashtable$Entry@8937} "dfs.datanode.dns.nameserver" -> "default"
  309 = {Hashtable$Entry@8938} "yarn.nodemanager.resource.memory-mb" -> "25600"
  310 = {Hashtable$Entry@8939} "yarn.log.server.web-service.url" -> "http://headnodehost:8188/ws/v1/applicationhistory"
  311 = {Hashtable$Entry@8940} "iocachedisabled.fs.wasb.impl" -> "org.apache.hadoop.fs.azure.NativeAzureFileSystem"
  312 = {Hashtable$Entry@8941} "mapreduce.task.userlog.limit.kb" -> "0"
  313 = {Hashtable$Entry@8942} "hadoop.security.crypto.codec.classes.aes.ctr.nopadding" -> "org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec,org.apache.hadoop.crypto.JceAesCtrCryptoCodec"
  314 = {Hashtable$Entry@8943} "fs.azure.account.keyprovider.abs0jar.blob.core.windows.net" -> "org.apache.hadoop.fs.azure.ShellDecryptionKeyProvider"
  315 = {Hashtable$Entry@8944} "mapreduce.reduce.speculative" -> "false"
  316 = {Hashtable$Entry@8945} "yarn.nodemanager.container-monitor.interval-ms" -> "3000"
  317 = {Hashtable$Entry@8946} "yarn.resourcemanager.admin.address.rm2" -> "hn1-chen-a.5ezjvsrqexeutmhucloqhdd3le.bx.internal.cloudapp.net:8141"
  318 = {Hashtable$Entry@8947} "yarn.node-labels.fs-store.impl.class" -> "org.apache.hadoop.yarn.nodelabels.FileSystemNodeLabelsStore"
  319 = {Hashtable$Entry@8948} "yarn.resourcemanager.admin.address.rm1" -> "hn0-chen-a.5ezjvsrqexeutmhucloqhdd3le.bx.internal.cloudapp.net:8141"
  320 = {Hashtable$Entry@8949} "net.topology.script.file.name" -> "/etc/hadoop/conf/topology_script.py"
  321 = {Hashtable$Entry@8950} "dfs.replication.max" -> "50"
  322 = {Hashtable$Entry@8951} "dfs.replication" -> "3"
  323 = {Hashtable$Entry@8952} "yarn.nodemanager.kill-escape.launch-command-line" -> "slider-agent,LLAP"
  324 = {Hashtable$Entry@8953} "dfs.ha.fencing.methods" -> "shell(/bin/true)"
  325 = {Hashtable$Entry@8954} "yarn.client.failover-retries" -> "0"
  326 = {Hashtable$Entry@8955} "yarn.nodemanager.resource.cpu-vcores" -> "7"
  327 = {Hashtable$Entry@8956} "mapreduce.jobhistory.recovery.enable" -> "true"
  328 = {Hashtable$Entry@8957} "nfs.exports.allowed.hosts" -> "* rw"
  329 = {Hashtable$Entry@8958} "yarn.sharedcache.checksum.algo.impl" -> "org.apache.hadoop.yarn.sharedcache.ChecksumSHA256Impl"
  330 = {Hashtable$Entry@8959} "mapreduce.reduce.shuffle.memory.limit.percent" -> "0.25"
  331 = {Hashtable$Entry@8960} "file.replication" -> "1"
  332 = {Hashtable$Entry@8961} "mapreduce.job.reduce.shuffle.consumer.plugin.class" -> "org.apache.hadoop.mapreduce.task.reduce.Shuffle"
  333 = {Hashtable$Entry@8962} "yarn.app.mapreduce.am.log.level" -> "INFO"
  334 = {Hashtable$Entry@8963} "yarn.nodemanager.webapp.rest-csrf.enabled" -> "false"
  335 = {Hashtable$Entry@8964} "mapreduce.job.jvm.numtasks" -> "1"
  336 = {Hashtable$Entry@9097} "dfs.datanode.fsdatasetcache.max.threads.per.volume" -> "4"
  337 = {Hashtable$Entry@9100} "mapreduce.am.max-attempts" -> "5"
  338 = {Hashtable$Entry@9103} "mapreduce.shuffle.connection-keep-alive.timeout" -> "5"
  339 = {Hashtable$Entry@9105} "hadoop.fuse.timer.period" -> "5"
  340 = {Hashtable$Entry@9107} "mapreduce.job.reduces" -> "1"
  341 = {Hashtable$Entry@9109} "hadoop.security.group.mapping.ldap.connection.timeout.ms" -> "60000"
  342 = {Hashtable$Entry@9112} "yarn.app.mapreduce.am.job.task.listener.thread-count" -> "30"
  343 = {Hashtable$Entry@9115} "yarn.resourcemanager.store.class" -> "org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore"
  344 = {Hashtable$Entry@9118} "dfs.client.retry.policy.enabled" -> "false"
  345 = {Hashtable$Entry@9120} "s3native.replication" -> "3"
  346 = {Hashtable$Entry@9123} "mapreduce.tasktracker.reduce.tasks.maximum" -> "2"
  347 = {Hashtable$Entry@9126} "fs.permissions.umask-mode" -> "022"
  348 = {Hashtable$Entry@9129} "mapreduce.cluster.local.dir" -> "/mnt/resource/hadoop/mapred/local"
  349 = {Hashtable$Entry@9132} "mapreduce.client.output.filter" -> "FAILED"
  350 = {Hashtable$Entry@9135} "yarn.nodemanager.pmem-check-enabled" -> "false"
  351 = {Hashtable$Entry@9137} "dfs.client.failover.connection.retries.on.timeouts" -> "0"
  352 = {Hashtable$Entry@9140} "mapreduce.jobtracker.instrumentation" -> "org.apache.hadoop.mapred.JobTrackerMetricsInst"
  353 = {Hashtable$Entry@9143} "ftp.replication" -> "3"
  354 = {Hashtable$Entry@9145} "yarn.timeline-service.webapp.rest-csrf.methods-to-ignore" -> "GET,OPTIONS,HEAD"
  355 = {Hashtable$Entry@9148} "hadoop.security.group.mapping.ldap.search.attr.member" -> "member"
  356 = {Hashtable$Entry@9151} "yarn.timeline-service.sqldb-store.connection-url" -> "jdbc:sqlserver://wcsakzm1tn.database.windows.net;databaseName=v36e8a8d6e5aee44b57a442e21ce8c620b0AmbariDb;sendStringParametersAsUnicode=false;trustServerCertificate=false;encrypt=true;hostNameInCertificate=*.database.windows.net;"
  357 = {Hashtable$Entry@9154} "fs.s3a.max.total.tasks" -> "5"
  358 = {Hashtable$Entry@9156} "dfs.namenode.replication.work.multiplier.per.iteration" -> "2"
  359 = {Hashtable$Entry@9158} "yarn.resourcemanager.fs.state-store.num-retries" -> "0"
  360 = {Hashtable$Entry@9160} "yarn.timeline-service.leveldb-state-store.path" -> "/hadoop/yarn/timeline"
  361 = {Hashtable$Entry@9163} "yarn.scheduler.capacity.ordering-policy.priority-utilization.underutilized-preemption.enabled" -> "false"
  362 = {Hashtable$Entry@9165} "yarn.resourcemanager.resource-tracker.address" -> "hn0-chen-a.5ezjvsrqexeutmhucloqhdd3le.bx.internal.cloudapp.net:8025"
  363 = {Hashtable$Entry@9168} "mapreduce.tasktracker.outofband.heartbeat" -> "false"
  364 = {Hashtable$Entry@9170} "dfs.namenode.edits.dir" -> "${dfs.namenode.name.dir}"
  365 = {Hashtable$Entry@9173} "dfs.https.port" -> "50470"
  366 = {Hashtable$Entry@9176} "yarn.resourcemanager.webapp.rest-csrf.methods-to-ignore" -> "GET,OPTIONS,HEAD"
  367 = {Hashtable$Entry@9178} "yarn.resourcemanager.scheduler.monitor.enable" -> "true"
  368 = {Hashtable$Entry@9181} "fs.trash.checkpoint.interval" -> "0"
  369 = {Hashtable$Entry@9183} "hadoop.registry.zk.retry.times" -> "5"
  370 = {Hashtable$Entry@9185} "dfs.client.read.shortcircuit.streams.cache.expiry.ms" -> "300000"
  371 = {Hashtable$Entry@9188} "yarn.timeline-service.leveldb-timeline-store.start-time-write-cache-size" -> "10000"
  372 = {Hashtable$Entry@9191} "s3.stream-buffer-size" -> "4096"
  373 = {Hashtable$Entry@9194} "fs.s3a.connection.maximum" -> "15"
  374 = {Hashtable$Entry@9197} "hadoop.security.dns.log-slow-lookups.enabled" -> "false"
  375 = {Hashtable$Entry@9199} "file.client-write-packet-size" -> "65536"
  376 = {Hashtable$Entry@9202} "yarn.log-aggregation.file-controller.TFile.class" -> "org.apache.hadoop.yarn.logaggregation.filecontroller.tfile.LogAggregationTFileController"
  377 = {Hashtable$Entry@9205} "mapreduce.tasktracker.healthchecker.script.timeout" -> "600000"
  378 = {Hashtable$Entry@9208} "hadoop.shell.missing.defaultFs.warning" -> "false"
  379 = {Hashtable$Entry@9210} "dfs.namenode.fs-limits.max-directory-items" -> "1048576"
  380 = {Hashtable$Entry@9213} "mapreduce.tasktracker.taskcontroller" -> "org.apache.hadoop.mapred.DefaultTaskController"
  381 = {Hashtable$Entry@9216} "dfs.namenode.path.based.cache.block.map.allocation.percent" -> "0.25"
  382 = {Hashtable$Entry@9219} "fs.s3a.impl" -> "org.apache.hadoop.fs.s3a.S3AFileSystem"
  383 = {Hashtable$Entry@9222} "yarn.nodemanager.windows-container.memory-limit.enabled" -> "false"
  384 = {Hashtable$Entry@9224} "dfs.namenode.checkpoint.dir" -> "/hadoop/hdfs/namesecondary"
  385 = {Hashtable$Entry@9227} "iocacheenabled.fs.AbstractFileSystem.wasb.impl" -> "com.qubole.rubix.hadoop2.CachingAbstractNativeAzureFileSystem"
  386 = {Hashtable$Entry@9230} "yarn.nodemanager.remote-app-log-dir" -> "/app-logs"
  387 = {Hashtable$Entry@9233} "mapreduce.reduce.shuffle.retry-delay.max.ms" -> "60000"
  388 = {Hashtable$Entry@9235} "io.map.index.interval" -> "128"
  389 = {Hashtable$Entry@9238} "dfs.namenode.replication.interval" -> "3"
  390 = {Hashtable$Entry@9240} "dfs.client.block.write.replace-datanode-on-failure.enable" -> "true"
  391 = {Hashtable$Entry@9242} "mapreduce.admin.user.env" -> "LD_LIBRARY_PATH=/usr/hdp/${hdp.version}/hadoop/lib/native:/usr/hdp/${hdp.version}/hadoop/lib/native/Linux-amd64-64:./mr-framework/hadoop/lib/native:./mr-framework/hadoop/lib/native/Linux-amd64-64"
  392 = {Hashtable$Entry@9245} "hadoop.ssl.server.conf" -> "ssl-server.xml"
  393 = {Hashtable$Entry@9248} "dfs.nameservices" -> "mycluster"
  394 = {Hashtable$Entry@9251} "hadoop.rpc.socket.factory.class.default" -> "org.apache.hadoop.net.StandardSocketFactory"
  395 = {Hashtable$Entry@9254} "dfs.client.socket.send.buffer.size" -> "0"
  396 = {Hashtable$Entry@9257} "yarn.app.mapreduce.client.max-retries" -> "3"
  397 = {Hashtable$Entry@9260} "yarn.nodemanager.address" -> "0.0.0.0:30050"
  398 = {Hashtable$Entry@9263} "dfs.namenode.lifeline.handler.ratio" -> "0.10"
  399 = {Hashtable$Entry@9266} "yarn.resourcemanager.max-log-aggregation-diagnostics-in-memory" -> "10"
  400 = {Hashtable$Entry@9271} "dfs.datanode.max.transfer.threads" -> "1024"
  401 = {Hashtable$Entry@9272} "ha.failover-controller.graceful-fence.rpc-timeout.ms" -> "5000"
  402 = {Hashtable$Entry@9273} "hadoop.proxyuser.livy.hosts" -> "*"
  403 = {Hashtable$Entry@9274} "dfs.datanode.ipc.address" -> "0.0.0.0:30020"
  404 = {Hashtable$Entry@9275} "yarn.resourcemanager.delayed.delegation-token.removal-interval-ms" -> "30000"
  405 = {Hashtable$Entry@9276} "hadoop.proxyuser.hcat.groups" -> "*"
  406 = {Hashtable$Entry@9277} "dfs.namenode.kerberos.principal.pattern" -> "*"
  407 = {Hashtable$Entry@9278} "yarn.timeline-service.enabled" -> "false"
  408 = {Hashtable$Entry@9279} "fs.azure.account.oauth.provider.type" -> "org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider"
  409 = {Hashtable$Entry@9280} "dfs.client.cached.conn.retry" -> "3"
  410 = {Hashtable$Entry@9281} "ipc.maximum.data.length" -> "67108864"
  411 = {Hashtable$Entry@9282} "dfs.namenode.backup.http-address" -> "0.0.0.0:50105"
  412 = {Hashtable$Entry@9283} "mapreduce.tasktracker.report.address" -> "127.0.0.1:0"
  413 = {Hashtable$Entry@9284} "dfs.namenode.checkpoint.period" -> "21600"
  414 = {Hashtable$Entry@9285} "hadoop.security.group.mapping.providers.combined" -> "true"
  415 = {Hashtable$Entry@9286} "dfs.datanode.shared.file.descriptor.paths" -> "/dev/shm,/tmp"
  416 = {Hashtable$Entry@9287} "fs.AbstractFileSystem.abfs.impl" -> "org.apache.hadoop.fs.azurebfs.Abfs"
  417 = {Hashtable$Entry@9288} "dfs.http.policy" -> "HTTP_ONLY"
  418 = {Hashtable$Entry@9289} "hadoop.security.groups.cache.warn.after.ms" -> "5000"
  419 = {Hashtable$Entry@9290} "hadoop.security.auth_to_local" -> "DEFAULT"
  420 = {Hashtable$Entry@9291} "io.compression.codec.lzo.class" -> "com.hadoop.compression.lzo.LzoCodec"
  421 = {Hashtable$Entry@9292} "yarn.resourcemanager.amlauncher.log.command" -> "false"
  422 = {Hashtable$Entry@9293} "fs.azure.account.key.abs0execution.blob.core.windows.net" -> "MIIB/modified->ZIhvcNAQcDoIIB7jCCAeoCAQAxggFdMIIBWQIBADBBMC0xKzApBgNVBAMTImRiZW5jcnlwdGlvbi5oZGluc2lnaHRzZXJ2aWNlcy5uZXQCEBSsRje2BBWjQSEZm1RPfpwwDQYJKoZIhvcNAQEBBQAEggEAkFSFrcDHsbQ9SuxtmbXQIxXTOjvVFFTfQwJTXSNbOq+5JRMhKkr9r4ELEDeRyt1lBFq/+NeDsi4lE8EBrIPsr5dYomAYaBVrrmfhzfXOhPkbvd0VuoX7cnzooL0wbVaDj14UiGFZA7VU+CjPUvwf1wlgCmbxqPvAChcu44rgMtEOzbLOHg0XiYeEE+cQGIv/gXptvGo1K2jXaJbFx6Te+C8gtblF8gc7gkoXcNXs6lBrZa6o6d6HkKZJVbfkX3nCkfisMsbp1ae9Hz3FcXPn6Sg4V2FlzUQl2MDtLIy+3jChYkjEM3vtu8TqkoV7vuDPU4miK5cNw0+jwwreUFlUoTCBgwYJKoZIhvcNAQcBMBQGCCqGSIb3DQMHBAhX2sDFDfXUyYBgP3O0TGE/rHXPBYJFyVLh41Q5g7EMTFkjf9dEYSIN7BrTGyPpkrQKnqH5N4yvyr6ciGLW729XRCX+nJgAzbiY4XYW9zL16/E9TMjs5SD87AwdKMex7yUR1YANRL+Rkgog"
  423 = {Hashtable$Entry@9294} "yarn.resourcemanager.fs.state-store.retry-interval-ms" -> "1000"
  424 = {Hashtable$Entry@9295} "dfs.namenode.fs-limits.max-xattrs-per-inode" -> "32"
  425 = {Hashtable$Entry@9296} "yarn.resourcemanager.zk-acl" -> "world:anyone:rwcda"
  426 = {Hashtable$Entry@9297} "dfs.datanode.transfer.socket.send.buffer.size" -> "0"
  427 = {Hashtable$Entry@9298} "dfs.namenode.support.allow.format" -> "true"
  428 = {Hashtable$Entry@9299} "yarn.sharedcache.app-checker.class" -> "org.apache.hadoop.yarn.server.sharedcachemanager.RemoteAppChecker"
  429 = {Hashtable$Entry@9300} "yarn.timeline-service.entity-group-fs-store.retain-seconds" -> "604800"
  430 = {Hashtable$Entry@9301} "dfs.namenode.checkpoint.max-retries" -> "3"
  431 = {Hashtable$Entry@9302} "yarn.resourcemanager.fs.state-store.retry-policy-spec" -> "2000, 500"
  432 = {Hashtable$Entry@9303} "fs.s3a.fast.upload" -> "true"
  433 = {Hashtable$Entry@9304} "mapreduce.job.committer.setup.cleanup.needed" -> "true"
  434 = {Hashtable$Entry@9305} "dfs.datanode.cache.revocation.polling.ms" -> "500"
  435 = {Hashtable$Entry@9306} "mapreduce.job.end-notification.retry.attempts" -> "0"
  436 = {Hashtable$Entry@9307} "yarn.resourcemanager.state-store.max-completed-applications" -> "${yarn.resourcemanager.max-completed-applications}"
  437 = {Hashtable$Entry@9308} "mapreduce.map.output.compress" -> "false"
  438 = {Hashtable$Entry@9436} "yarn.timeline-service.generic-application-history.store-class" -> "org.apache.hadoop.yarn.server.applicationhistoryservice.NullApplicationHistoryStore"
  439 = {Hashtable$Entry@9439} "mapreduce.jobhistory.cleaner.enable" -> "true"
  440 = {Hashtable$Entry@9442} "mapreduce.job.running.reduce.limit" -> "0"
  441 = {Hashtable$Entry@9444} "io.seqfile.local.dir" -> "${hadoop.tmp.dir}/io/local"
  442 = {Hashtable$Entry@9447} "dfs.blockreport.split.threshold" -> "1000000"
  443 = {Hashtable$Entry@9450} "mapreduce.reduce.shuffle.read.timeout" -> "180000"
  444 = {Hashtable$Entry@9453} "mapreduce.job.queuename" -> "default"
  445 = {Hashtable$Entry@9456} "hive.metastore.client.socket.timeout" -> "1800"
  446 = {Hashtable$Entry@9459} "yarn.nodemanager.logaggregation.threadpool-size-max" -> "100"
  447 = {Hashtable$Entry@9462} "dfs.datanode.scan.period.hours" -> "504"
  448 = {Hashtable$Entry@9465} "yarn.nodemanager.runtime.linux.docker.allowed-container-networks" -> "host,none,bridge"
  449 = {Hashtable$Entry@9468} "ipc.client.connect.max.retries" -> "50"
  450 = {Hashtable$Entry@9471} "io.seqfile.lazydecompress" -> "true"
  451 = {Hashtable$Entry@9473} "yarn.app.mapreduce.am.staging-dir" -> "/mapreducestaging"
  452 = {Hashtable$Entry@9476} "yarn.timeline-service.leveldb-timeline-store.read-cache-size" -> "104857600"
  453 = {Hashtable$Entry@9479} "yarn.nodemanager.linux-container-executor.resources-handler.class" -> "org.apache.hadoop.yarn.server.nodemanager.util.DefaultLCEResourcesHandler"
  454 = {Hashtable$Entry@9482} "yarn.app.mapreduce.client.job.retry-interval" -> "2000"
  455 = {Hashtable$Entry@9485} "io.file.buffer.size" -> "65536"
  456 = {Hashtable$Entry@9488} "yarn.resourcemanager.webapp.cross-origin.enabled" -> "true"
  457 = {Hashtable$Entry@9490} "yarn.resourcemanager.am-rm-tokens.master-key-rolling-interval-secs" -> "86400"
  458 = {Hashtable$Entry@9493} "ha.zookeeper.parent-znode" -> "/hadoop-ha"
  459 = {Hashtable$Entry@9496} "mapreduce.tasktracker.indexcache.mb" -> "10"
  460 = {Hashtable$Entry@9499} "tfile.io.chunk.size" -> "1048576"
  461 = {Hashtable$Entry@9502} "yarn.resourcemanager.work-preserving-recovery.scheduling-wait-ms" -> "10000"
  462 = {Hashtable$Entry@9505} "yarn.timeline-service.keytab" -> "/etc/krb5.keytab"
  463 = {Hashtable$Entry@9508} "yarn.timeline-service.generic-application-history.save-non-am-container-meta-info" -> "true"
  464 = {Hashtable$Entry@9510} "yarn.node-labels.enabled" -> "false"
  465 = {Hashtable$Entry@9512} "yarn.acl.enable" -> "false"
  466 = {Hashtable$Entry@9514} "hadoop.security.group.mapping.ldap.directory.search.timeout" -> "10000"
  467 = {Hashtable$Entry@9516} "mapreduce.application.classpath" -> "$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/lib/*:$PWD/mr-framework/hadoop/share/hadoop/tools/lib/*:/usr/hdp/${hdp.version}/hadoop/lib/hadoop-lzo-0.6.0.${hdp.version}.jar:/etc/hadoop/conf/secure:/usr/hdp/current/ext/hadoop/*"
  468 = {Hashtable$Entry@9519} "yarn.timeline-service.version" -> "1.5"
  469 = {Hashtable$Entry@9522} "dfs.webhdfs.socket.read-timeout" -> "60s"
  470 = {Hashtable$Entry@9525} "mapreduce.job.token.tracking.ids.enabled" -> "false"
  471 = {Hashtable$Entry@9527} "dfs.datanode.block-pinning.enabled" -> "false"
  472 = {Hashtable$Entry@9529} "mapreduce.map.output.compress.codec" -> "org.apache.hadoop.io.compress.DefaultCodec"
  473 = {Hashtable$Entry@9532} "yarn.sharedcache.enabled" -> "false"
  474 = {Hashtable$Entry@9534} "s3.replication" -> "3"
  475 = {Hashtable$Entry@9537} "yarn.timeline-service.http-authentication.type" -> "simple"
  476 = {Hashtable$Entry@9540} "hadoop.registry.zk.root" -> "/registry"
  477 = {Hashtable$Entry@9543} "tfile.fs.input.buffer.size" -> "262144"
  478 = {Hashtable$Entry@9546} "dfs.cluster.administrators" -> " hdfs"
  479 = {Hashtable$Entry@9549} "ha.failover-controller.graceful-fence.connection.retries" -> "1"
  480 = {Hashtable$Entry@9552} "net.topology.script.number.args" -> "100"
  481 = {Hashtable$Entry@9554} "fs.s3n.multipart.uploads.block.size" -> "67108864"
  482 = {Hashtable$Entry@9557} "iocache.cache.nonlocal.blocks.from.remote" -> "true"
  483 = {Hashtable$Entry@9559} "yarn.sharedcache.admin.thread-count" -> "1"
  484 = {Hashtable$Entry@9561} "dfs.ha.zkfc.nn.http.timeout.ms" -> "20000"
  485 = {Hashtable$Entry@9564} "yarn.nodemanager.recovery.dir" -> "/var/log/hadoop-yarn/nodemanager/recovery-state"
  486 = {Hashtable$Entry@9567} "hadoop.ssl.enabled" -> "false"
  487 = {Hashtable$Entry@9569} "fs.AbstractFileSystem.ftp.impl" -> "org.apache.hadoop.fs.ftp.FtpFs"
  488 = {Hashtable$Entry@9572} "yarn.timeline-service.handler-thread-count" -> "10"
  489 = {Hashtable$Entry@9574} "yarn.nodemanager.container-metrics.unregister-delay-ms" -> "60000"
  490 = {Hashtable$Entry@9577} "hadoop.caller.context.enabled" -> "true"
  491 = {Hashtable$Entry@9579} "dfs.namenode.reject-unresolved-dn-topology-mapping" -> "false"
  492 = {Hashtable$Entry@9581} "mapreduce.jobhistory.recovery.store.class" -> "org.apache.hadoop.mapreduce.v2.hs.HistoryServerLeveldbStateStoreService"
  493 = {Hashtable$Entry@9584} "yarn.nodemanager.log.retain-seconds" -> "604800"
  494 = {Hashtable$Entry@9587} "yarn.resourcemanager.admin.address" -> "hn0-chen-a.5ezjvsrqexeutmhucloqhdd3le.bx.internal.cloudapp.net:8141"
  495 = {Hashtable$Entry@9590} "yarn.resourcemanager.recovery.enabled" -> "true"
  496 = {Hashtable$Entry@9592} "dfs.client.slow.io.warning.threshold.ms" -> "30000"
  497 = {Hashtable$Entry@9595} "yarn.resourcemanager.ha.automatic-failover.zk-base-path" -> "/yarn-leader-election"
  498 = {Hashtable$Entry@9598} "hadoop.proxyuser.livy.groups" -> "*"
  499 = {Hashtable$Entry@9601} "fs.AbstractFileSystem.viewfs.impl" -> "org.apache.hadoop.fs.viewfs.ViewFs"
  500 = {Hashtable$Entry@9606} "mapreduce.tasktracker.dns.interface" -> "default"
  501 = {Hashtable$Entry@9607} "mapreduce.jobtracker.handler.count" -> "10"
  502 = {Hashtable$Entry@9608} "dfs.blockreport.initialDelay" -> "0"
  503 = {Hashtable$Entry@9609} "fs.AbstractFileSystem.hdfs.impl" -> "org.apache.hadoop.fs.Hdfs"
  504 = {Hashtable$Entry@9610} "dfs.namenode.top.enabled" -> "true"
  505 = {Hashtable$Entry@9611} "dfs.namenode.retrycache.expirytime.millis" -> "600000"
  506 = {Hashtable$Entry@9612} "dfs.webhdfs.rest-csrf.browser-useragents-regex" -> "^Mozilla.*,^Opera.*"
  507 = {Hashtable$Entry@9613} "mapreduce.job.speculative.speculative-cap-total-tasks" -> "0.01"
  508 = {Hashtable$Entry@9614} "dfs.client.failover.sleep.max.millis" -> "15000"
  509 = {Hashtable$Entry@9615} "yarn.timeline-service.generic-application-history.max-applications" -> "10000"
  510 = {Hashtable$Entry@9616} "yarn.sharedcache.nm.uploader.thread-count" -> "20"
  511 = {Hashtable$Entry@9617} "yarn.nodemanager.log-container-debug-info.enabled" -> "true"
  512 = {Hashtable$Entry@9618} "fs.AbstractFileSystem.s3a.impl" -> "org.apache.hadoop.fs.s3a.S3A"
  513 = {Hashtable$Entry@9619} "dfs.namenode.blocks.per.postponedblocks.rescan" -> "10000"
  514 = {Hashtable$Entry@9620} "yarn.timeline-service.webapp.rest-csrf.custom-header" -> "X-XSRF-Header"
  515 = {Hashtable$Entry@9621} "yarn.resourcemanager.max-completed-applications" -> "1000"
  516 = {Hashtable$Entry@9622} "hadoop.proxyuser.oozie.groups" -> "*"
  517 = {Hashtable$Entry@9623} "yarn.nodemanager.log-dirs" -> "/mnt/resource/hadoop/yarn/log"
  518 = {Hashtable$Entry@9624} "iocacheenabled.fs.AbstractFileSystem.wasbs.impl" -> "com.qubole.rubix.hadoop2.CachingAbstractNativeAzureSecureFileSystem"
  519 = {Hashtable$Entry@9625} "mapreduce.jobhistory.webapp.rest-csrf.methods-to-ignore" -> "GET,OPTIONS,HEAD"
  520 = {Hashtable$Entry@9626} "dfs.client.failover.sleep.base.millis" -> "500"
  521 = {Hashtable$Entry@9627} "yarn.nodemanager.linux-container-executor.nonsecure-mode.user-pattern" -> "^[_.A-Za-z0-9][-@_.A-Za-z0-9]{0,255}?[$]?$"
  522 = {Hashtable$Entry@9628} "dfs.hosts.exclude" -> "/etc/hadoop/conf/dfs.exclude"
  523 = {Hashtable$Entry@9629} "dfs.default.chunk.view.size" -> "32768"
  524 = {Hashtable$Entry@9630} "dfs.client.read.shortcircuit" -> "true"
  525 = {Hashtable$Entry@9631} "ftp.blocksize" -> "67108864"
  526 = {Hashtable$Entry@9632} "mapreduce.job.acl-modify-job" -> " "
  527 = {Hashtable$Entry@9633} "fs.defaultFS" -> "wasbs://chen-ablc-2019-08-06t17-19-54-826z@abs0execution.blob.core.windows.net"
  528 = {Hashtable$Entry@9634} "hadoop.http.filter.initializers" -> "org.apache.hadoop.http.lib.StaticUserWebFilter,org.apache.hadoop.security.HttpCrossOriginFilterInitializer"
  529 = {Hashtable$Entry@9635} "fs.s3n.multipart.copy.block.size" -> "5368709120"
  530 = {Hashtable$Entry@9636} "mapreduce.map.java.opts" -> "-Xmx2560M -Xms2560M -Djava.net.preferIPv4Stack=true -XX:NewRatio=8 -XX:+UseNUMA -XX:+UseParallelGC"
  531 = {Hashtable$Entry@9637} "fs.adl.impl" -> "org.apache.hadoop.fs.adl.AdlFileSystem"
  532 = {Hashtable$Entry@9638} "fs.adl.oauth2.access.token.provider.type" -> "ClientCredential"
  533 = {Hashtable$Entry@9639} "yarn.resourcemanager.connect.max-wait.ms" -> "7200000"
  534 = {Hashtable$Entry@9640} "dfs.ha.namenodes.mycluster" -> "nn1,nn2"
  535 = {Hashtable$Entry@9641} "yarn.timeline-service.entity-group-fs-store.scan-interval-seconds" -> "15"
  536 = {Hashtable$Entry@9642} "dfs.balancer.max-no-move-interval" -> "60000"
  537 = {Hashtable$Entry@9643} "hadoop.security.group.mapping.ldap.ssl" -> "false"
  538 = {Hashtable$Entry@9644} "dfs.namenode.max.extra.edits.segments.retained" -> "10000"
  539 = {Hashtable$Entry@9645} "dfs.namenode.https-address" -> "0.0.0.0:50470"
  540 = {Hashtable$Entry@9782} "yarn.nodemanager.aux-services" -> "mapreduce_shuffle,spark_shuffle,spark2_shuffle"
  541 = {Hashtable$Entry@9785} "dfs.block.scanner.volume.bytes.per.second" -> "1048576"
  542 = {Hashtable$Entry@9788} "yarn.sharedcache.store.class" -> "org.apache.hadoop.yarn.server.sharedcachemanager.store.InMemorySCMStore"
  543 = {Hashtable$Entry@9791} "yarn.resourcemanager.bind-host" -> "0.0.0.0"
  544 = {Hashtable$Entry@9794} "dfs.namenode.decommission.blocks.per.interval" -> "500000"
  545 = {Hashtable$Entry@9797} "dfs.mover.max-no-move-interval" -> "60000"
  546 = {Hashtable$Entry@9799} "iocacheenabled.fs.wasbs.impl" -> "com.qubole.rubix.hadoop2.CachingNativeAzureSecureFileSystem"
  547 = {Hashtable$Entry@9802} "yarn.fail-fast" -> "false"
  548 = {Hashtable$Entry@9804} "yarn.resourcemanager.admin.client.thread-count" -> "1"
  549 = {Hashtable$Entry@9807} "hadoop.security.kms.client.encrypted.key.cache.size" -> "500"
  550 = {Hashtable$Entry@9810} "yarn.app.mapreduce.shuffle.log.separate" -> "true"
  551 = {Hashtable$Entry@9813} "dfs.encrypt.data.transfer.cipher.suites" -> "AES/CTR/NoPadding"
  552 = {Hashtable$Entry@9816} "yarn.timeline-service.entity-group-fs-store.group-id-plugin-classes" -> "org.apache.spark.deploy.history.yarn.plugin.SparkATSPlugin,org.apache.tez.dag.history.logging.ats.TimelineCachePluginImpl"
  553 = {Hashtable$Entry@9819} "ipc.client.kill.max" -> "10"
  554 = {Hashtable$Entry@9822} "hadoop.security.group.mapping.ldap.search.filter.group" -> "(objectClass=group)"
  555 = {Hashtable$Entry@9825} "fs.AbstractFileSystem.file.impl" -> "org.apache.hadoop.fs.local.LocalFs"
  556 = {Hashtable$Entry@9828} "hive.exec.scratchdir" -> "hdfs://mycluster/tmp/hive"
  557 = {Hashtable$Entry@9831} "fs.azure.io.read.tolerate.concurrent.append" -> "true"
  558 = {Hashtable$Entry@9833} "hadoop.http.authentication.kerberos.keytab" -> "${user.home}/hadoop.keytab"
  559 = {Hashtable$Entry@9836} "yarn.timeline-service.sqldb-store.connection-username" -> "v36e8a8d6e5aee44b57a442e21ce8c620b0AmbariDbLogin@wcsakzm1tn.database.windows.net"
  560 = {Hashtable$Entry@9839} "yarn.client.nodemanager-connect.max-wait-ms" -> "60000"
  561 = {Hashtable$Entry@9841} "mapreduce.job.map.output.collector.class" -> "org.apache.hadoop.mapred.MapTask$MapOutputBuffer"
  562 = {Hashtable$Entry@9844} "dfs.namenode.path.based.cache.retry.interval.ms" -> "30000"
  563 = {Hashtable$Entry@9847} "hadoop.security.uid.cache.secs" -> "14400"
  564 = {Hashtable$Entry@9850} "mapreduce.map.cpu.vcores" -> "1"
  565 = {Hashtable$Entry@9852} "yarn.log-aggregation.retain-check-interval-seconds" -> "-1"
  566 = {Hashtable$Entry@9855} "mapreduce.map.log.level" -> "INFO"
  567 = {Hashtable$Entry@9858} "mapred.child.java.opts" -> "-Xmx200m"
  568 = {Hashtable$Entry@9861} "fs.AbstractFileSystem.abfss.impl" -> "org.apache.hadoop.fs.azurebfs.Abfss"
  569 = {Hashtable$Entry@9864} "yarn.app.mapreduce.am.hard-kill-timeout-ms" -> "10000"
  570 = {Hashtable$Entry@9866} "hadoop.registry.zk.session.timeout.ms" -> "60000"
  571 = {Hashtable$Entry@9868} "fs.azure.io.copyblob.retry.max.retries" -> "60"
  572 = {Hashtable$Entry@9871} "mapreduce.job.running.map.limit" -> "0"
  573 = {Hashtable$Entry@9874} "yarn.sharedcache.store.in-memory.initial-delay-mins" -> "10"
  574 = {Hashtable$Entry@9876} "yarn.timeline-service.entity-group-fs-store.cleaner-interval-seconds" -> "3600"
  575 = {Hashtable$Entry@9879} "yarn.sharedcache.client-server.thread-count" -> "50"
  576 = {Hashtable$Entry@9882} "yarn.nodemanager.local-cache.max-files-per-directory" -> "8192"
  577 = {Hashtable$Entry@9885} "dfs.datanode.cached-dfsused.check.interval.ms" -> "600000"
  578 = {Hashtable$Entry@9888} "dfs.https.server.keystore.resource" -> "ssl-server.xml"
  579 = {Hashtable$Entry@9891} "mapreduce.jobtracker.taskcache.levels" -> "2"
  580 = {Hashtable$Entry@9894} "dfs.webhdfs.rest-csrf.enabled" -> "false"
  581 = {Hashtable$Entry@9896} "dfs.webhdfs.ugi.expire.after.access" -> "600000"
  582 = {Hashtable$Entry@9898} "dfs.datanode.handler.count" -> "10"
  583 = {Hashtable$Entry@9900} "dfs.balancer.block-move.timeout" -> "0"
  584 = {Hashtable$Entry@9902} "ha.failover-controller.active-standby-elector.zk.op.retries" -> "120"
  585 = {Hashtable$Entry@9905} "yarn.resourcemanager.display.per-user-apps" -> "false"
  586 = {Hashtable$Entry@9907} "s3native.blocksize" -> "67108864"
  587 = {Hashtable$Entry@9910} "iocacheenabled.fs.wasb.impl" -> "com.qubole.rubix.hadoop2.CachingNativeAzureFileSystem"
  588 = {Hashtable$Entry@9913} "yarn.log-aggregation.file-controller.IndexedFormat.class" -> "org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController"
  589 = {Hashtable$Entry@9916} "mapreduce.client.completion.pollinterval" -> "5000"
  590 = {Hashtable$Entry@9919} "dfs.stream-buffer-size" -> "4096"
  591 = {Hashtable$Entry@9922} "fs.s3a.socket.send.buffer" -> "8192"
  592 = {Hashtable$Entry@9924} "dfs.webhdfs.rest-csrf.custom-header" -> "X-XSRF-HEADER"
  593 = {Hashtable$Entry@9927} "dfs.namenode.delegation.key.update-interval" -> "86400000"
  594 = {Hashtable$Entry@9930} "mapreduce.job.maps" -> "2"
  595 = {Hashtable$Entry@9932} "fs.AbstractFileSystem.swebhdfs.impl" -> "org.apache.hadoop.fs.SWebHdfs"
  596 = {Hashtable$Entry@9935} "mapreduce.job.acl-view-job" -> " "
  597 = {Hashtable$Entry@9938} "fs.s3a.readahead.range" -> "64K"
  598 = {Hashtable$Entry@9941} "dfs.namenode.enable.retrycache" -> "true"
  599 = {Hashtable$Entry@9943} "yarn.resourcemanager.connect.retry-interval.ms" -> "30000"
  600 = {Hashtable$Entry@10046} "dfs.namenode.shared.edits.dir" -> "qjournal://zk0-chen-a.5ezjvsrqexeutmhucloqhdd3le.bx.internal.cloudapp.net:8485;zk2-chen-a.5ezjvsrqexeutmhucloqhdd3le.bx.internal.cloudapp.net:8485;zk6-chen-a.5ezjvsrqexeutmhucloqhdd3le.bx.internal.cloudapp.net:8485/mycluster"
  601 = {Hashtable$Entry@10049} "yarn.timeline-service.leveldb-timeline-store.ttl-interval-ms" -> "43200000"
  602 = {Hashtable$Entry@10052} "fs.s3a.multipart.threshold" -> "2147483647"
  603 = {Hashtable$Entry@10055} "dfs.namenode.decommission.interval" -> "30"
  604 = {Hashtable$Entry@10058} "mapreduce.shuffle.max.connections" -> "0"
  605 = {Hashtable$Entry@10061} "hadoop.shell.safely.delete.limit.num.files" -> "100"
  606 = {Hashtable$Entry@10064} "fs.azure.sas.expiry.period" -> "90d"
  607 = {Hashtable$Entry@10067} "yarn.log-aggregation-enable" -> "true"
  608 = {Hashtable$Entry@10070} "dfs.client-write-packet-size" -> "65536"
  609 = {Hashtable$Entry@10073} "dfs.client.file-block-storage-locations.timeout.millis" -> "1000"
  610 = {Hashtable$Entry@10076} "dfs.client.block.write.retries" -> "3"
  611 = {Hashtable$Entry@10079} "mapreduce.jobtracker.expire.trackers.interval" -> "600000"
  612 = {Hashtable$Entry@10082} "mapreduce.task.io.sort.factor" -> "100"
  613 = {Hashtable$Entry@10084} "hadoop.security.dns.log-slow-lookups.threshold.ms" -> "1000"
  614 = {Hashtable$Entry@10086} "ha.health-monitor.sleep-after-disconnect.ms" -> "1000"
  615 = {Hashtable$Entry@10088} "ha.zookeeper.session-timeout.ms" -> "5000"
  616 = {Hashtable$Entry@10091} "hive.server2.transport.mode" -> "http"
  617 = {Hashtable$Entry@10094} "yarn.nodemanager.linux-container-executor.nonsecure-mode.limit-users" -> "true"
  618 = {Hashtable$Entry@10096} "fs.azure.account.oauth2.client.id" -> "6358f0cd-ce12-4e89-be99-66b16637880e"
  619 = {Hashtable$Entry@10099} "dfs.datanode.transfer.socket.recv.buffer.size" -> "0"
  620 = {Hashtable$Entry@10101} "dfs.support.append" -> "true"
  621 = {Hashtable$Entry@10103} "yarn.resourcemanager.webapp.rest-csrf.custom-header" -> "X-XSRF-Header"
  622 = {Hashtable$Entry@10106} "mapreduce.input.fileinputformat.list-status.num-threads" -> "1"
  623 = {Hashtable$Entry@10109} "io.skip.checksum.errors" -> "false"
  624 = {Hashtable$Entry@10112} "dfs.namenode.lease-recheck-interval-ms" -> "2000"
  625 = {Hashtable$Entry@10115} "yarn.resourcemanager.scheduler.client.thread-count" -> "50"
  626 = {Hashtable$Entry@10118} "mapreduce.cluster.administrators" -> " hadoop"
  627 = {Hashtable$Entry@10121} "dfs.namenode.safemode.extension" -> "0"
  628 = {Hashtable$Entry@10123} "mapreduce.jobhistory.move.thread-count" -> "3"
  629 = {Hashtable$Entry@10125} "yarn.resourcemanager.zk-state-store.parent-path" -> "/rmstore"
  630 = {Hashtable$Entry@10128} "yarn.timeline-service.client.fd-retain-secs" -> "90"
  631 = {Hashtable$Entry@10131} "ipc.client.idlethreshold" -> "8000"
  632 = {Hashtable$Entry@10134} "yarn.sharedcache.cleaner.initial-delay-mins" -> "10"
  633 = {Hashtable$Entry@10137} "yarn.timeline-service.entity-group-fs-store.cache-store-class" -> "org.apache.hadoop.yarn.server.timeline.MemoryTimelineStore"
  634 = {Hashtable$Entry@10140} "dfs.namenode.accesstime.precision" -> "0"
  635 = {Hashtable$Entry@10142} "mapreduce.task.profile.params" -> "-agentlib:hprof=cpu=samples,heap=sites,force=n,thread=y,verbose=n,file=%s"
  636 = {Hashtable$Entry@10145} "fs.azure.saskey.usecontainersaskeyforallaccess" -> "true"
  637 = {Hashtable$Entry@10147} "mapreduce.jobhistory.keytab" -> "/etc/security/keytab/jhs.service.keytab"
  638 = {Hashtable$Entry@10150} "dfs.datanode.hdfs-blocks-metadata.enabled" -> "false"
  639 = {Hashtable$Entry@10152} "yarn.scheduler.minimum-allocation-mb" -> "512"
  640 = {Hashtable$Entry@10155} "yarn.resourcemanager.container-tokens.master-key-rolling-interval-secs" -> "86400"
  641 = {Hashtable$Entry@10158} "mapreduce.reduce.shuffle.fetch.retry.interval-ms" -> "1000"
  642 = {Hashtable$Entry@10160} "yarn.timeline-service.entity-group-fs-store.app-cache-size" -> "10"
  643 = {Hashtable$Entry@10162} "hadoop.user.group.static.mapping.overrides" -> "dr.who=;"
  644 = {Hashtable$Entry@10165} "hadoop.security.kms.client.encrypted.key.cache.low-watermark" -> "0.3f"
  645 = {Hashtable$Entry@10168} "dfs.datanode.directoryscan.interval" -> "21600"
  646 = {Hashtable$Entry@10171} "fs.s3a.connection.ssl.enabled" -> "true"
  647 = {Hashtable$Entry@10173} "fs.abfss.impl" -> "org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem"
  648 = {Hashtable$Entry@10176} "yarn.node-labels.fs-store.retry-policy-spec" -> "2000, 500"
  649 = {Hashtable$Entry@10179} "yarn.nodemanager.runtime.linux.docker.capabilities" -> "CHOWN,DAC_OVERRIDE,FSETID,FOWNER,MKNOD,NET_RAW,SETGID,SETUID,SETFCAP,SETPCAP,NET_BIND_SERVICE,SYS_CHROOT,KILL,AUDIT_WRITE"
  650 = {Hashtable$Entry@10182} "yarn.timeline-service.webapp.rest-csrf.enabled" -> "true"
  651 = {Hashtable$Entry@10184} "fs.AbstractFileSystem.webhdfs.impl" -> "org.apache.hadoop.fs.WebHdfs"
  652 = {Hashtable$Entry@10187} "dfs.xframe.enabled" -> "true"
  653 = {Hashtable$Entry@10189} "yarn.resourcemanager.scheduler.monitor.policies" -> "org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy"
  654 = {Hashtable$Entry@10000} "ipc.server.listen.queue.size" -> "128"
  655 = {Hashtable$Entry@10001} "rpc.metrics.quantile.enable" -> "false"
  656 = {Hashtable$Entry@10002} "yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds" -> "3600"
  657 = {Hashtable$Entry@10003} "mapreduce.jobtracker.persist.jobstatus.dir" -> "/jobtracker/jobsInfo"
  658 = {Hashtable$Entry@10004} "dfs.domain.socket.path" -> "/var/lib/hadoop-hdfs/dn_socket"
  659 = {Hashtable$Entry@10005} "yarn.client.nodemanager-client-async.thread-pool-max-size" -> "500"
  660 = {Hashtable$Entry@10006} "hadoop.security.group.mapping" -> "org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback"
  661 = {Hashtable$Entry@10007} "yarn.nodemanager.runtime.linux.docker.default-container-network" -> "host"
  662 = {Hashtable$Entry@10008} "yarn.resourcemanager.system-metrics-publisher.enabled" -> "true"
  663 = {Hashtable$Entry@10009} "dfs.namenode.name.dir" -> "/hadoop/hdfs/namenode"
  664 = {Hashtable$Entry@10010} "yarn.am.liveness-monitor.expiry-interval-ms" -> "600000"
  665 = {Hashtable$Entry@10211} "yarn.nm.liveness-monitor.expiry-interval-ms" -> "600000"
  666 = {Hashtable$Entry@10213} "ftp.bytes-per-checksum" -> "512"
  667 = {Hashtable$Entry@10216} "yarn.sharedcache.nested-level" -> "3"
  668 = {Hashtable$Entry@10219} "dfs.namenode.max.objects" -> "0"
  669 = {Hashtable$Entry@10222} "hadoop.http.logs.enabled" -> "true"
  670 = {Hashtable$Entry@10224} "ha.zookeeper.quorum" -> "zk0-chen-a.5ezjvsrqexeutmhucloqhdd3le.bx.internal.cloudapp.net:2181,zk2-chen-a.5ezjvsrqexeutmhucloqhdd3le.bx.internal.cloudapp.net:2181,zk6-chen-a.5ezjvsrqexeutmhucloqhdd3le.bx.internal.cloudapp.net:2181"
  671 = {Hashtable$Entry@10227} "mapreduce.job.emit-timeline-data" -> "false"
  672 = {Hashtable$Entry@10230} "io.compression.codecs" -> "org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.SnappyCodec,com.hadoop.compression.lzo.LzoCodec,com.hadoop.compression.lzo.LzopCodec"
  673 = {Hashtable$Entry@10233} "mapreduce.map.memory.mb" -> "3072"
  674 = {Hashtable$Entry@10236} "yarn.client.nodemanager-connect.retry-interval-ms" -> "10000"
  675 = {Hashtable$Entry@10239} "hadoop.http.cross-origin.max-age" -> "1800"
  676 = {Hashtable$Entry@10242} "dfs.namenode.edits.journal-plugin.qjournal" -> "org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager"
  677 = {Hashtable$Entry@10245} "yarn.nodemanager.container.stderr.pattern" -> "{*stderr*,*STDERR*}"
  678 = {Hashtable$Entry@10248} "mapreduce.tasktracker.healthchecker.interval" -> "60000"
  679 = {Hashtable$Entry@10251} "fs.azure.secure.mode" -> "false"
  680 = {Hashtable$Entry@10253} "fs.azure.account.auth.type" -> "OAuth"
  681 = {Hashtable$Entry@10256} "yarn.nodemanager.aux-services.spark_shuffle.classpath" -> "/usr/hdp/${hdp.version}/spark/aux/*"
  682 = {Hashtable$Entry@10259} "nfs.wtmax" -> "1048576"
  683 = {Hashtable$Entry@10262} "yarn.timeline-service.leveldb-timeline-store.start-time-read-cache-size" -> "10000"
  684 = {Hashtable$Entry@10264} "mapreduce.job.speculative.retry-after-no-speculate" -> "1000"
  685 = {Hashtable$Entry@10267} "hadoop.registry.zk.connection.timeout.ms" -> "15000"
  686 = {Hashtable$Entry@10270} "yarn.resourcemanager.address" -> "hn0-chen-a.5ezjvsrqexeutmhucloqhdd3le.bx.internal.cloudapp.net:8050"
  687 = {Hashtable$Entry@10273} "ipc.client.rpc-timeout.ms" -> "0"
  688 = {Hashtable$Entry@10275} "dfs.cachereport.intervalMsec" -> "10000"
  689 = {Hashtable$Entry@10277} "mapreduce.task.skip.start.attempts" -> "2"
  690 = {Hashtable$Entry@10280} "fs.s3a.socket.recv.buffer" -> "8192"
  691 = {Hashtable$Entry@10283} "yarn.resourcemanager.zk-timeout-ms" -> "10000"
  692 = {Hashtable$Entry@10285} "yarn.timeline-service.entity-group-fs-store.summary-store" -> "org.apache.hadoop.yarn.server.timeline.SqlDBTimelineStore"
  693 = {Hashtable$Entry@10288} "hadoop.security.groups.cache.background.reload.threads" -> "3"
  694 = {Hashtable$Entry@10290} "dfs.namenode.checkpoint.edits.dir" -> "${dfs.namenode.checkpoint.dir}"
  695 = {Hashtable$Entry@10293} "hadoop.hdfs.configuration.version" -> "1"
  696 = {Hashtable$Entry@10296} "hadoop.proxyuser.hive.groups" -> "*"
  697 = {Hashtable$Entry@10299} "yarn.sharedcache.cleaner.resource-sleep-ms" -> "0"
  698 = {Hashtable$Entry@10301} "mapreduce.map.skip.maxrecords" -> "0"
  699 = {Hashtable$Entry@10303} "yarn.resourcemanager.system-metrics-publisher.dispatcher.pool-size" -> "10"
  700 = {Hashtable$Entry@10307} "dfs.datanode.available-space-volume-choosing-policy.balanced-space-threshold" -> "10737418240"
  701 = {Hashtable$Entry@10308} "nfs.allow.insecure.ports" -> "true"
  702 = {Hashtable$Entry@10309} "mapreduce.jobtracker.system.dir" -> "${hadoop.tmp.dir}/mapred/system"
  703 = {Hashtable$Entry@10310} "yarn.timeline-service.hostname" -> "0.0.0.0"
  704 = {Hashtable$Entry@10311} "hadoop.registry.rm.enabled" -> "false"
  705 = {Hashtable$Entry@10312} "mapreduce.job.reducer.preempt.delay.sec" -> "0"
  706 = {Hashtable$Entry@10313} "hadoop.proxyuser.oozie.hosts" -> "*"
  707 = {Hashtable$Entry@10314} "mapreduce.shuffle.ssl.enabled" -> "false"
  708 = {Hashtable$Entry@10315} "yarn.nodemanager.vmem-pmem-ratio" -> "2.1"
  709 = {Hashtable$Entry@10316} "yarn.nodemanager.container-manager.thread-count" -> "20"
  710 = {Hashtable$Entry@10317} "yarn.timeline-service.entity-file.fs-support-append" -> "false"
  711 = {Hashtable$Entry@10318} "dfs.encrypt.data.transfer" -> "false"
  712 = {Hashtable$Entry@10319} "dfs.block.access.key.update.interval" -> "600"
  713 = {Hashtable$Entry@10320} "yarn.resourcemanager.cluster-id" -> "yarn-cluster"
  714 = {Hashtable$Entry@10321} "dfs.internal.nameservices" -> "mycluster"
  715 = {Hashtable$Entry@10322} "hadoop.tmp.dir" -> "/tmp/hadoop-${user.name}"
  716 = {Hashtable$Entry@10323} "dfs.namenode.audit.loggers" -> "default"
  717 = {Hashtable$Entry@10324} "fs.AbstractFileSystem.har.impl" -> "org.apache.hadoop.fs.HarFs"
  718 = {Hashtable$Entry@10325} "yarn.nodemanager.localizer.cache.target-size-mb" -> "10240"
  719 = {Hashtable$Entry@10326} "yarn.app.mapreduce.shuffle.log.backups" -> "0"
  720 = {Hashtable$Entry@10327} "yarn.http.policy" -> "HTTP_ONLY"
  721 = {Hashtable$Entry@10328} "dfs.client.short.circuit.replica.stale.threshold.ms" -> "1800000"
  722 = {Hashtable$Entry@10329} "yarn.timeline-service.webapp.https.address" -> "headnodehost:8190"
  723 = {Hashtable$Entry@10330} "yarn.resourcemanager.amlauncher.thread-count" -> "50"
  724 = {Hashtable$Entry@10331} "yarn.nodemanager.aux-services.spark2_shuffle.class" -> "org.apache.spark.network.yarn.YarnShuffleService"
  725 = {Hashtable$Entry@10332} "yarn.timeline-service.webapp.xfs-filter.xframe-options" -> "SAMEORIGIN"
  726 = {Hashtable$Entry@10333} "yarn.log.server.url" -> "http://hn0-chen-a.5ezjvsrqexeutmhucloqhdd3le.bx.internal.cloudapp.net:19888/jobhistory/logs"
  727 = {Hashtable$Entry@10453} "mapreduce.jobtracker.persist.jobstatus.hours" -> "1"
  728 = {Hashtable$Entry@10456} "hive.server2.thrift.http.path" -> "/"
  729 = {Hashtable$Entry@10459} "tfile.fs.output.buffer.size" -> "262144"
  730 = {Hashtable$Entry@10462} "dfs.namenode.checkpoint.check.period" -> "60"
  731 = {Hashtable$Entry@10465} "dfs.datanode.dns.interface" -> "default"
  732 = {Hashtable$Entry@10468} "fs.ftp.host.port" -> "21"
  733 = {Hashtable$Entry@10471} "mapreduce.task.io.sort.mb" -> "1228"
  734 = {Hashtable$Entry@10474} "dfs.namenode.inotify.max.events.per.rpc" -> "1000"
  735 = {Hashtable$Entry@10477} "yarn.resourcemanager.webapp.xfs-filter.xframe-options" -> "SAMEORIGIN"
  736 = {Hashtable$Entry@10479} "hadoop.security.group.mapping.ldap.search.attr.group.name" -> "cn"
  737 = {Hashtable$Entry@10482} "fs.abfss.impl.disable.cache" -> "true"
  738 = {Hashtable$Entry@10485} "hadoop.proxyuser.hcat.hosts" -> "*"
  739 = {Hashtable$Entry@10488} "hadoop.security.group.mapping.ldap.read.timeout.ms" -> "60000"
  740 = {Hashtable$Entry@10491} "dfs.namenode.avoid.read.stale.datanode" -> "true"
  741 = {Hashtable$Entry@10494} "mapreduce.output.fileoutputformat.compress.type" -> "BLOCK"
  742 = {Hashtable$Entry@10497} "fs.azure.local.sas.key.mode" -> "false"
  743 = {Hashtable$Entry@10500} "mapreduce.reduce.skip.proc.count.autoincr" -> "true"
  744 = {Hashtable$Entry@10502} "dfs.datanode.http.address" -> "0.0.0.0:30075"
  745 = {Hashtable$Entry@10505} "file.bytes-per-checksum" -> "512"
  746 = {Hashtable$Entry@10508} "mapreduce.job.userlog.retain.hours" -> "24"
  747 = {Hashtable$Entry@10511} "dfs.image.compress" -> "false"
  748 = {Hashtable$Entry@10513} "mapreduce.reduce.java.opts" -> "-Xmx2560M -Xms2560M -Djava.net.preferIPv4Stack=true -XX:NewRatio=8 -XX:+UseNUMA -XX:+UseParallelGC"
  749 = {Hashtable$Entry@10516} "dfs.journalnode.edits.dir" -> "/hadoop/hdfs/journal"
  750 = {Hashtable$Entry@10519} "yarn.resourcemanager.zk-address" -> "zk0-chen-a.5ezjvsrqexeutmhucloqhdd3le.bx.internal.cloudapp.net:2181,zk2-chen-a.5ezjvsrqexeutmhucloqhdd3le.bx.internal.cloudapp.net:2181,zk6-chen-a.5ezjvsrqexeutmhucloqhdd3le.bx.internal.cloudapp.net:2181"
  751 = {Hashtable$Entry@10522} "ha.health-monitor.check-interval.ms" -> "1000"
  752 = {Hashtable$Entry@10524} "dfs.permissions.enabled" -> "false"
  753 = {Hashtable$Entry@10526} "yarn.resourcemanager.resource-tracker.client.thread-count" -> "50"
  754 = {Hashtable$Entry@10529} "dfs.client.domain.socket.data.traffic" -> "false"
  755 = {Hashtable$Entry@10531} "dfs.image.compression.codec" -> "org.apache.hadoop.io.compress.DefaultCodec"
  756 = {Hashtable$Entry@10534} "dfs.block.access.token.enable" -> "true"
  757 = {Hashtable$Entry@10536} "dfs.datanode.address" -> "0.0.0.0:30010"
  758 = {Hashtable$Entry@10539} "yarn.resourcemanager.scheduler.address.rm2" -> "hn1-chen-a.5ezjvsrqexeutmhucloqhdd3le.bx.internal.cloudapp.net:8030"
  759 = {Hashtable$Entry@10542} "yarn.resourcemanager.scheduler.address.rm1" -> "hn0-chen-a.5ezjvsrqexeutmhucloqhdd3le.bx.internal.cloudapp.net:8030"
  760 = {Hashtable$Entry@10545} "mapreduce.reduce.input.buffer.percent" -> "0.0"
  761 = {Hashtable$Entry@10548} "yarn.timeline-service.sqldb-store.connection-password" -> "vtBKJuQnHkMaNa5IBkZJlNrCA6XqZNutYsUFSh19buq9GlpRwyWPkKkw6CGcZXik2Z2KtgzxT6Ny4elEupgm2xOmUTRo9kGedXkdYVnH2lIW2O"
  762 = {Hashtable$Entry@10551} "yarn.nodemanager.linux-container-executor.cgroups.strict-resource-usage" -> "false"
  763 = {Hashtable$Entry@10553} "yarn.nodemanager.aux-services.spark2_shuffle.classpath" -> "/usr/hdp/${hdp.version}/spark2/aux/*"
  764 = {Hashtable$Entry@10556} "mapreduce.tasktracker.local.dir.minspacestart" -> "0"
  765 = {Hashtable$Entry@10559} "dfs.blockreport.intervalMsec" -> "21600000"
  766 = {Hashtable$Entry@10562} "mapreduce.shuffle.transferTo.allowed" -> "true"
  767 = {Hashtable$Entry@10564} "ha.health-monitor.rpc-timeout.ms" -> "45000"
  768 = {Hashtable$Entry@10567} "dfs.datanode.bp-ready.timeout" -> "20"
  769 = {Hashtable$Entry@10570} "dfs.client.failover.connection.retries" -> "0"
  770 = {Hashtable$Entry@10572} "dfs.namenode.kerberos.internal.spnego.principal" -> "${dfs.web.authentication.kerberos.principal}"
  771 = {Hashtable$Entry@10575} "yarn.scheduler.maximum-allocation-mb" -> "25600"
  772 = {Hashtable$Entry@10578} "yarn.resourcemanager.leveldb-state-store.path" -> "${hadoop.tmp.dir}/yarn/system/rmstore"
  773 = {Hashtable$Entry@10581} "mapreduce.task.files.preserve.failedtasks" -> "false"
  774 = {Hashtable$Entry@10583} "yarn.nodemanager.delete.thread-count" -> "4"
  775 = {Hashtable$Entry@10586} "mapreduce.output.fileoutputformat.compress.codec" -> "org.apache.hadoop.io.compress.DefaultCodec"
  776 = {Hashtable$Entry@10588} "yarn.resourcemanager.monitor.capacity.preemption.natural_termination_factor" -> "1"
  777 = {Hashtable$Entry@10590} "map.sort.class" -> "org.apache.hadoop.util.QuickSort"
  778 = {Hashtable$Entry@10593} "mapreduce.jobhistory.jobname.limit" -> "50"
  779 = {Hashtable$Entry@10595} "mapreduce.job.classloader" -> "false"
  780 = {Hashtable$Entry@10597} "hadoop.registry.zk.retry.ceiling.ms" -> "60000"
  781 = {Hashtable$Entry@10599} "fs.azure.shellkeyprovider.script" -> "/usr/lib/hdinsight-common/scripts/decrypt.sh"
  782 = {Hashtable$Entry@10602} "mapreduce.jobtracker.tasktracker.maxblacklists" -> "4"
  783 = {Hashtable$Entry@10604} "dfs.blocksize" -> "134217728"
  784 = {Hashtable$Entry@10607} "io.seqfile.compress.blocksize" -> "1000000"
  785 = {Hashtable$Entry@10610} "mapreduce.task.profile.maps" -> "0-2"
  786 = {Hashtable$Entry@10613} "mapreduce.jobtracker.staging.root.dir" -> "${hadoop.tmp.dir}/mapred/staging"
  787 = {Hashtable$Entry@10616} "yarn.nodemanager.localizer.cache.cleanup.interval-ms" -> "600000"
  788 = {Hashtable$Entry@10619} "mapreduce.jobtracker.http.address" -> "0.0.0.0:50030"
  789 = {Hashtable$Entry@10622} "hadoop.proxyuser.hive.hosts" -> "*"
  790 = {Hashtable$Entry@10625} "dfs.client.mmap.cache.timeout.ms" -> "3600000"
  791 = {Hashtable$Entry@10628} "hadoop.http.cross-origin.allowed-origins" -> "*"
  792 = {Hashtable$Entry@10630} "yarn.timeline-service.client.fd-flush-interval-secs" -> "5"
  793 = {Hashtable$Entry@10633} "hadoop.security.java.secure.random.algorithm" -> "SHA1PRNG"
  794 = {Hashtable$Entry@10636} "fs.client.resolve.remote.symlinks" -> "true"
  795 = {Hashtable$Entry@10639} "mapreduce.tasktracker.local.dir.minspacekill" -> "0"
  796 = {Hashtable$Entry@10642} "nfs.mountd.port" -> "4242"
  797 = {Hashtable$Entry@10645} "yarn.nodemanager.disk-health-checker.min-healthy-disks" -> "0.25"
  798 = {Hashtable$Entry@10648} "mapreduce.tasktracker.taskmemorymanager.monitoringinterval" -> "5000"
  799 = {Hashtable$Entry@10651} "yarn.resourcemanager.nodes.exclude-path" -> "/etc/hadoop/conf/yarn.exclude"
  800 = {Hashtable$Entry@10655} "dfs.namenode.resource.du.reserved" -> "104857600"
  801 = {Hashtable$Entry@10656} "iocachedisabled.fs.wasbs.impl" -> "org.apache.hadoop.fs.azure.NativeAzureFileSystem"
  802 = {Hashtable$Entry@10657} "mapreduce.job.end-notification.retry.interval" -> "1000"
  803 = {Hashtable$Entry@10658} "mapreduce.jobhistory.loadedjobs.cache.size" -> "5"
  804 = {Hashtable$Entry@10659} "dfs.client.datanode-restart.timeout" -> "30"
  805 = {Hashtable$Entry@10660} "fs.s3a.fast.upload.active.blocks" -> "4"
  806 = {Hashtable$Entry@10661} "yarn.nodemanager.local-dirs" -> "/mnt/resource/hadoop/yarn/local"
  807 = {Hashtable$Entry@10662} "dfs.datanode.block.id.layout.upgrade.threads" -> "12"
  808 = {Hashtable$Entry@10663} "yarn.timeline-service.webapp.address" -> "headnodehost:8188"
  809 = {Hashtable$Entry@10664} "hadoop.registry.jaas.context" -> "Client"
  810 = {Hashtable$Entry@10665} "mapreduce.jobhistory.address" -> "headnodehost:10020"
  811 = {Hashtable$Entry@10666} "mapreduce.jobtracker.persist.jobstatus.active" -> "true"
  812 = {Hashtable$Entry@10667} "ipc.server.log.slow.rpc" -> "false"
  813 = {Hashtable$Entry@10668} "file.blocksize" -> "67108864"
  814 = {Hashtable$Entry@10669} "dfs.datanode.readahead.bytes" -> "4194304"
  815 = {Hashtable$Entry@10670} "yarn.sharedcache.cleaner.period-mins" -> "1440"
  816 = {Hashtable$Entry@10671} "yarn.timeline-service.entity-group-fs-store.leveldb-cache-read-cache-size" -> "10485760"
  817 = {Hashtable$Entry@10672} "fs.s3a.block.size" -> "32M"
  818 = {Hashtable$Entry@10673} "dfs.namenode.http-address" -> "0.0.0.0:50070"
  819 = {Hashtable$Entry@10674} "dfs.webhdfs.rest-csrf.methods-to-ignore" -> "GET,OPTIONS,HEAD,TRACE"
  820 = {Hashtable$Entry@10675} "ipc.client.ping" -> "true"
  821 = {Hashtable$Entry@10676} "yarn.resourcemanager.leveldb-state-store.compaction-interval-secs" -> "3600"
  822 = {Hashtable$Entry@10677} "yarn.resourcemanager.configuration.provider-class" -> "org.apache.hadoop.yarn.LocalConfigurationProvider"
  823 = {Hashtable$Entry@10678} "yarn.nodemanager.recovery.enabled" -> "true"
  824 = {Hashtable$Entry@10679} "yarn.resourcemanager.hostname" -> "hn0-chen-a.5ezjvsrqexeutmhucloqhdd3le.bx.internal.cloudapp.net"
  825 = {Hashtable$Entry@10680} "yarn.resourcemanager.webapp.address.rm2" -> "hn1-chen-a.5ezjvsrqexeutmhucloqhdd3le.bx.internal.cloudapp.net:8088"
  826 = {Hashtable$Entry@10681} "yarn.resourcemanager.webapp.address.rm1" -> "hn0-chen-a.5ezjvsrqexeutmhucloqhdd3le.bx.internal.cloudapp.net:8088"
  827 = {Hashtable$Entry@10682} "fs.s3n.multipart.uploads.enabled" -> "false"
  828 = {Hashtable$Entry@10683} "yarn.nodemanager.linux-container-executor.group" -> "hadoop"
  829 = {Hashtable$Entry@10684} "mapreduce.jobhistory.always-scan-user-dir" -> "True"
  830 = {Hashtable$Entry@10685} "yarn.timeline-service.entity-group-fs-store.group-id-plugin-classpath" -> "/usr/hdp/current/spark-client/hdpLib/*"
  831 = {Hashtable$Entry@10686} "yarn.nodemanager.log-aggregation.num-log-files-per-app" -> "336"
  832 = {Hashtable$Entry@10687} "dfs.namenode.fs-limits.max-component-length" -> "255"
  833 = {Hashtable$Entry@10688} "ha.failover-controller.cli-check.rpc-timeout.ms" -> "20000"
  834 = {Hashtable$Entry@10689} "ftp.client-write-packet-size" -> "65536"
  835 = {Hashtable$Entry@10690} "mapreduce.reduce.shuffle.parallelcopies" -> "30"
  836 = {Hashtable$Entry@10691} "mapreduce.jobhistory.principal" -> "jhs/_HOST@REALM.TLD"
  837 = {Hashtable$Entry@10692} "hadoop.http.authentication.simple.anonymous.allowed" -> "true"
  838 = {Hashtable$Entry@10693} "yarn.log-aggregation.retain-seconds" -> "604800"
  839 = {Hashtable$Entry@10694} "yarn.nodemanager.windows-container.cpu-limit.enabled" -> "false"
  840 = {Hashtable$Entry@10695} "yarn.timeline-service.http-authentication.simple.anonymous.allowed" -> "true"
  841 = {Hashtable$Entry@10696} "yarn.timeline-service.sqldb-store.driver" -> "com.microsoft.sqlserver.jdbc.SQLServerDriver"
  842 = {Hashtable$Entry@10697} "dfs.namenode.secondary.https-address" -> "0.0.0.0:50091"
  843 = {Hashtable$Entry@10698} "mapreduce.job.ubertask.maxreduces" -> "1"
  844 = {Hashtable$Entry@10699} "fs.s3a.connection.establish.timeout" -> "5000"
  845 = {Hashtable$Entry@10700} "yarn.nodemanager.health-checker.interval-ms" -> "135000"
  846 = {Hashtable$Entry@10701} "dfs.namenode.fs-limits.max-xattr-size" -> "16384"
  847 = {Hashtable$Entry@10702} "fs.s3a.multipart.purge" -> "false"
  848 = {Hashtable$Entry@10703} "hadoop.security.kms.client.encrypted.key.cache.num.refill.threads" -> "2"
  849 = {Hashtable$Entry@10704} "fs.AbstractFileSystem.adl.impl" -> "org.apache.hadoop.fs.adl.Adl"
  850 = {Hashtable$Entry@10705} "yarn.timeline-service.store-class" -> "org.apache.hadoop.yarn.server.timeline.HdInsightEntityGroupFSTimelineStore"
  851 = {Hashtable$Entry@10706} "mapreduce.shuffle.transfer.buffer.size" -> "131072"
  852 = {Hashtable$Entry@10707} "yarn.resourcemanager.zk-num-retries" -> "1000"
  853 = {Hashtable$Entry@10708} "mapreduce.jobtracker.jobhistory.task.numberprogresssplits" -> "12"
  854 = {Hashtable$Entry@10709} "hive.server2.thrift.port" -> "10016"
  855 = {Hashtable$Entry@10710} "yarn.timeline-service.generic-application-history.hdinsight.filter-container-meta-info" -> "true"
  856 = {Hashtable$Entry@10711} "yarn.sharedcache.store.in-memory.staleness-period-mins" -> "10080"
  857 = {Hashtable$Entry@10712} "yarn.nodemanager.webapp.address" -> "0.0.0.0:30060"
  858 = {Hashtable$Entry@10713} "yarn.app.mapreduce.client-am.ipc.max-retries" -> "3"
  859 = {Hashtable$Entry@10714} "ipc.ping.interval" -> "60000"
  860 = {Hashtable$Entry@10715} "ha.failover-controller.new-active.rpc-timeout.ms" -> "60000"
  861 = {Hashtable$Entry@10716} "mapreduce.jobhistory.client.thread-count" -> "10"
  862 = {Hashtable$Entry@10717} "dfs.namenode.block-placement-policy.default.prefer-local-node" -> "true"
  863 = {Hashtable$Entry@10718} "fs.trash.interval" -> "0"
  864 = {Hashtable$Entry@10719} "mapreduce.fileoutputcommitter.algorithm.version" -> "2"
  865 = {Hashtable$Entry@10720} "mapreduce.reduce.skip.maxgroups" -> "0"
  866 = {Hashtable$Entry@10868} "dfs.namenode.top.windows.minutes" -> "1,5,25"
  867 = {Hashtable$Entry@10871} "mapreduce.reduce.memory.mb" -> "3072"
  868 = {Hashtable$Entry@10874} "yarn.nodemanager.health-checker.script.timeout-ms" -> "60000"
  869 = {Hashtable$Entry@10877} "dfs.datanode.du.reserved" -> "1073741824"
  870 = {Hashtable$Entry@10880} "dfs.namenode.resource.check.interval" -> "5000"
  871 = {Hashtable$Entry@10883} "mapreduce.client.progressmonitor.pollinterval" -> "1000"
  872 = {Hashtable$Entry@10886} "yarn.nodemanager.default-container-executor.log-dirs.permissions" -> "710"
  873 = {Hashtable$Entry@10889} "yarn.nodemanager.hostname" -> "0.0.0.0"
  874 = {Hashtable$Entry@10892} "yarn.resourcemanager.ha.enabled" -> "true"
  875 = {Hashtable$Entry@10895} "dfs.ha.log-roll.period" -> "120"
  876 = {Hashtable$Entry@10898} "yarn.scheduler.minimum-allocation-vcores" -> "1"
  877 = {Hashtable$Entry@10901} "dfs.client.block.write.replace-datanode-on-failure.best-effort" -> "false"
  878 = {Hashtable$Entry@10904} "yarn.app.mapreduce.am.container.log.limit.kb" -> "0"
  879 = {Hashtable$Entry@10906} "hadoop.http.authentication.signature.secret.file" -> "${user.home}/hadoop-http-auth-signature-secret"
  880 = {Hashtable$Entry@10909} "mapreduce.jobhistory.move.interval-ms" -> "180000"
  881 = {Hashtable$Entry@10912} "yarn.nodemanager.container-executor.class" -> "org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor"
  882 = {Hashtable$Entry@10915} "hadoop.security.authorization" -> "false"
  883 = {Hashtable$Entry@10917} "dfs.storage.policy.enabled" -> "true"
  884 = {Hashtable$Entry@10919} "dfs.datanode.https.address" -> "0.0.0.0:30475"
  885 = {Hashtable$Entry@10922} "yarn.nodemanager.localizer.address" -> "${yarn.nodemanager.hostname}:8040"
  886 = {Hashtable$Entry@10925} "dfs.namenode.replication.min" -> "1"
  887 = {Hashtable$Entry@10927} "mapreduce.jobhistory.recovery.store.fs.uri" -> "${hadoop.tmp.dir}/mapred/history/recoverystore"
  888 = {Hashtable$Entry@10930} "hive.metastore.warehouse.dir" -> "/hive/warehouse"
  889 = {Hashtable$Entry@10933} "fs.azure.account.oauth2.client.endpoint" -> "https://login.microsoftonline.com/modified->5ffc-43aa-b7d2-fb14d30c8bd3/oauth2/token"
  890 = {Hashtable$Entry@10936} "mapreduce.shuffle.connection-keep-alive.enable" -> "false"
  891 = {Hashtable$Entry@10938} "hadoop.security.group.mapping.ldap.search.group.hierarchy.levels" -> "0"
  892 = {Hashtable$Entry@10940} "dfs.namenode.top.num.users" -> "10"
  893 = {Hashtable$Entry@10943} "yarn.log-aggregation.file-formats" -> "IndexedFormat,TFile"
  894 = {Hashtable$Entry@10946} "hadoop.common.configuration.version" -> "0.23.0"
  895 = {Hashtable$Entry@10949} "yarn.app.mapreduce.task.container.log.backups" -> "0"
  896 = {Hashtable$Entry@10951} "dfs.namenode.fslock.fair" -> "false"
  897 = {Hashtable$Entry@10953} "hadoop.security.groups.negative-cache.secs" -> "30"
  898 = {Hashtable$Entry@10956} "dfs.ha.tail-edits.rolledits.timeout" -> "60"
  899 = {Hashtable$Entry@10959} "dfs.content-summary.limit" -> "5000"
  900 = {Hashtable$Entry@10963} "mapreduce.ifile.readahead" -> "true"
  901 = {Hashtable$Entry@10964} "yarn.nodemanager.resource.percentage-physical-cpu-limit" -> "80"
  902 = {Hashtable$Entry@10965} "yarn.resourcemanager.hostname.rm2" -> "hn1-chen-a.5ezjvsrqexeutmhucloqhdd3le.bx.internal.cloudapp.net"
  903 = {Hashtable$Entry@10966} "yarn.resourcemanager.hostname.rm1" -> "hn0-chen-a.5ezjvsrqexeutmhucloqhdd3le.bx.internal.cloudapp.net"
  904 = {Hashtable$Entry@10967} "mapreduce.job.max.split.locations" -> "10"
  905 = {Hashtable$Entry@10968} "dfs.datanode.max.locked.memory" -> "0"
  906 = {Hashtable$Entry@10969} "hadoop.registry.zk.quorum" -> "zk0-chen-a.5ezjvsrqexeutmhucloqhdd3le.bx.internal.cloudapp.net:2181,zk2-chen-a.5ezjvsrqexeutmhucloqhdd3le.bx.internal.cloudapp.net:2181,zk6-chen-a.5ezjvsrqexeutmhucloqhdd3le.bx.internal.cloudapp.net:2181"
  907 = {Hashtable$Entry@10970} "fs.s3a.threads.keepalivetime" -> "60"
  908 = {Hashtable$Entry@10971} "mapreduce.jobhistory.joblist.cache.size" -> "20000"
  909 = {Hashtable$Entry@10972} "mapreduce.job.end-notification.max.attempts" -> "5"
  910 = {Hashtable$Entry@10973} "dfs.image.transfer.timeout" -> "60000"
  911 = {Hashtable$Entry@10974} "dfs.client.read.shortcircuit.skip.checksum" -> "false"
  912 = {Hashtable$Entry@10975} "hadoop.security.groups.cache.background.reload" -> "false"
  913 = {Hashtable$Entry@10976} "nfs.rtmax" -> "1048576"
  914 = {Hashtable$Entry@10977} "dfs.namenode.edit.log.autoroll.check.interval.ms" -> "300000"
  915 = {Hashtable$Entry@10978} "mapreduce.reduce.shuffle.connect.timeout" -> "180000"
  916 = {Hashtable$Entry@10979} "dfs.datanode.failed.volumes.tolerated" -> "0"
  917 = {Hashtable$Entry@10980} "mapreduce.jobhistory.webapp.address" -> "headnodehost:19888"
  918 = {Hashtable$Entry@10981} "fs.s3a.connection.timeout" -> "200000"
  919 = {Hashtable$Entry@10982} "dfs.xframe.value" -> "SAMEORIGIN"
  920 = {Hashtable$Entry@10983} "dfs.client.mmap.retry.timeout.ms" -> "300000"
  921 = {Hashtable$Entry@10984} "dfs.datanode.data.dir.perm" -> "750"
  922 = {Hashtable$Entry@10985} "yarn.sharedcache.nm.uploader.replication.factor" -> "10"
  923 = {Hashtable$Entry@10986} "hadoop.http.authentication.token.validity" -> "36000"
  924 = {Hashtable$Entry@10987} "ipc.client.connect.max.retries.on.timeouts" -> "45"
  925 = {Hashtable$Entry@10988} "yarn.timeline-service.client.internal-timers-ttl-secs" -> "150"
  926 = {Hashtable$Entry@10989} "yarn.nodemanager.docker-container-executor.exec-name" -> "/usr/bin/docker"
  927 = {Hashtable$Entry@10990} "yarn.app.mapreduce.am.job.committer.cancel-timeout" -> "60000"
  928 = {Hashtable$Entry@10991} "dfs.ha.fencing.ssh.connect-timeout" -> "30000"
  929 = {Hashtable$Entry@10992} "mapreduce.reduce.log.level" -> "INFO"
  930 = {Hashtable$Entry@10993} "manage.include.files" -> "false"
  931 = {Hashtable$Entry@11112} "mapreduce.reduce.shuffle.merge.percent" -> "0.66"
  932 = {Hashtable$Entry@11115} "yarn.resourcemanager.monitor.capacity.preemption.max_wait_before_kill" -> "5000"
  933 = {Hashtable$Entry@11118} "ipc.client.fallback-to-simple-auth-allowed" -> "false"
  934 = {Hashtable$Entry@11120} "io.serializations" -> "org.apache.hadoop.io.serializer.WritableSerialization"
  935 = {Hashtable$Entry@11123} "fs.s3.block.size" -> "67108864"
  936 = {Hashtable$Entry@11126} "fs.abfs.impl.disable.cache" -> "true"
  937 = {Hashtable$Entry@11129} "yarn.nodemanager.linux-container-executor.nonsecure-mode.local-user" -> "nobody"
  938 = {Hashtable$Entry@11132} "fs.s3a.user.agent.prefix" -> "User-Agent: APN/1.0 Hortonworks/1.0 HDP/2.6.5.3008-11"
  939 = {Hashtable$Entry@11135} "hadoop.kerberos.kinit.command" -> "kinit"
  940 = {Hashtable$Entry@11138} "hadoop.security.kms.client.encrypted.key.cache.expiry" -> "43200000"
  941 = {Hashtable$Entry@11141} "yarn.resourcemanager.fs.state-store.uri" -> " "
  942 = {Hashtable$Entry@11144} "iocachedisabled.fs.AbstractFileSystem.wasbs.impl" -> "org.apache.hadoop.fs.azure.Wasbs"
  943 = {Hashtable$Entry@11147} "yarn.dispatcher.drain-events.timeout" -> "300000"
  944 = {Hashtable$Entry@11150} "yarn.admin.acl" -> "*"
  945 = {Hashtable$Entry@11153} "dfs.namenode.delegation.token.max-lifetime" -> "604800000"
  946 = {Hashtable$Entry@11156} "mapreduce.reduce.merge.inmem.threshold" -> "1000"
  947 = {Hashtable$Entry@11159} "dfs.webhdfs.socket.connect-timeout" -> "60s"
  948 = {Hashtable$Entry@11162} "net.topology.impl" -> "org.apache.hadoop.net.NetworkTopology"
  949 = {Hashtable$Entry@11165} "yarn.resourcemanager.ha.automatic-failover.enabled" -> "true"
  950 = {Hashtable$Entry@11168} "dfs.datanode.use.datanode.hostname" -> "false"
  951 = {Hashtable$Entry@11170} "dfs.heartbeat.interval" -> "3"
  952 = {Hashtable$Entry@11173} "yarn.resourcemanager.scheduler.class" -> "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler"
  953 = {Hashtable$Entry@11176} "io.map.index.skip" -> "0"
  954 = {Hashtable$Entry@11179} "dfs.namenode.handler.count" -> "100"
  955 = {Hashtable$Entry@11182} "yarn.resourcemanager.webapp.https.address" -> "hn0-chen-a.5ezjvsrqexeutmhucloqhdd3le.bx.internal.cloudapp.net:8090"
  956 = {Hashtable$Entry@11185} "yarn.nodemanager.admin-env" -> "MALLOC_ARENA_MAX=$MALLOC_ARENA_MAX"
  957 = {Hashtable$Entry@11188} "yarn.resourcemanager.address.rm2" -> "hn1-chen-a.5ezjvsrqexeutmhucloqhdd3le.bx.internal.cloudapp.net:8050"
  958 = {Hashtable$Entry@11191} "hadoop.security.crypto.cipher.suite" -> "AES/CTR/NoPadding"
  959 = {Hashtable$Entry@11194} "mapreduce.task.profile.map.params" -> "${mapreduce.task.profile.params}"
  960 = {Hashtable$Entry@11197} "yarn.resourcemanager.address.rm1" -> "hn0-chen-a.5ezjvsrqexeutmhucloqhdd3le.bx.internal.cloudapp.net:8050"
  961 = {Hashtable$Entry@11200} "mapreduce.jobtracker.jobhistory.block.size" -> "3145728"
  962 = {Hashtable$Entry@11203} "hadoop.security.crypto.buffer.size" -> "8192"
  963 = {Hashtable$Entry@11206} "yarn.nodemanager.aux-services.mapreduce_shuffle.class" -> "org.apache.hadoop.mapred.ShuffleHandler"
  964 = {Hashtable$Entry@11209} "fs.s3a.path.style.access" -> "false"
  965 = {Hashtable$Entry@11211} "dfs.datanode.disk.check.min.gap" -> "15m"
  966 = {Hashtable$Entry@11214} "mapreduce.cluster.acls.enabled" -> "false"
  967 = {Hashtable$Entry@11216} "mapreduce.application.framework.path" -> "/hdp/apps/${hdp.version}/mapreduce/mapreduce.tar.gz#mr-framework"
  968 = {Hashtable$Entry@11219} "yarn.sharedcache.uploader.server.address" -> "0.0.0.0:8046"
  969 = {Hashtable$Entry@11222} "fs.s3a.threads.max" -> "10"
  970 = {Hashtable$Entry@11225} "fs.har.impl.disable.cache" -> "true"
  971 = {Hashtable$Entry@11227} "mapreduce.tasktracker.map.tasks.maximum" -> "2"
  972 = {Hashtable$Entry@11230} "ipc.client.connect.timeout" -> "20000"
  973 = {Hashtable$Entry@11233} "yarn.nodemanager.remote-app-log-dir-suffix" -> "logs"
  974 = {Hashtable$Entry@11236} "fs.df.interval" -> "60000"
  975 = {Hashtable$Entry@11238} "hadoop.cache.data.dirprefix.list" -> "/mnt/iocache/"
  976 = {Hashtable$Entry@11241} "hadoop.util.hash.type" -> "murmur"
  977 = {Hashtable$Entry@11244} "mapreduce.jobhistory.minicluster.fixed.ports" -> "false"
  978 = {Hashtable$Entry@11246} "mapreduce.jobtracker.jobhistory.lru.cache.size" -> "5"
  979 = {Hashtable$Entry@11249} "yarn.app.mapreduce.shuffle.log.limit.kb" -> "0"
  980 = {Hashtable$Entry@11251} "dfs.client.failover.max.attempts" -> "15"
  981 = {Hashtable$Entry@11254} "dfs.client.use.datanode.hostname" -> "false"
  982 = {Hashtable$Entry@11256} "yarn.timeline-service.entity-group-fs-store.done-dir" -> "/atshistory/done"
  983 = {Hashtable$Entry@11259} "ha.zookeeper.acl" -> "world:anyone:rwcda"
  984 = {Hashtable$Entry@11262} "mapreduce.jobtracker.maxtasks.perjob" -> "-1"
  985 = {Hashtable$Entry@11265} "mapreduce.job.speculative.speculative-cap-running-tasks" -> "0.1"
  986 = {Hashtable$Entry@11268} "mapreduce.admin.map.child.java.opts" -> "-server -XX:NewRatio=8 -Djava.net.preferIPv4Stack=true -Dhdp.version=${hdp.version}"
  987 = {Hashtable$Entry@11271} "mapreduce.map.sort.spill.percent" -> "0.7"
  988 = {Hashtable$Entry@11274} "file.stream-buffer-size" -> "4096"
  989 = {Hashtable$Entry@11277} "yarn.resourcemanager.ha.automatic-failover.embedded" -> "true"
  990 = {Hashtable$Entry@11279} "hive.metastore.uris" -> "thrift://hn0-chen-a.5ezjvsrqexeutmhucloqhdd3le.bx.internal.cloudapp.net:9083,thrift://hn1-chen-a.5ezjvsrqexeutmhucloqhdd3le.bx.internal.cloudapp.net:9083"
  991 = {Hashtable$Entry@11282} "yarn.resourcemanager.nodemanager.minimum.version" -> "NONE"
  992 = {Hashtable$Entry@11285} "hadoop.fuse.connection.timeout" -> "300"
  993 = {Hashtable$Entry@11288} "dfs.datanode.disk.check.timeout" -> "10m"
  994 = {Hashtable$Entry@11291} "mapreduce.tasktracker.instrumentation" -> "org.apache.hadoop.mapred.TaskTrackerMetricsInst"
  995 = {Hashtable$Entry@11294} "yarn.nodemanager.aux-services.spark_shuffle.class" -> "org.apache.spark.network.yarn.YarnShuffleService"
  996 = {Hashtable$Entry@11297} "adl.http.timeout" -> "-1"
  997 = {Hashtable$Entry@11299} "io.seqfile.sorter.recordlimit" -> "1000000"
  998 = {Hashtable$Entry@11302} "yarn.sharedcache.webapp.address" -> "0.0.0.0:8788"
  999 = {Hashtable$Entry@11305} "hive.server2.thrift.http.port" -> "10002"
  1000 = {Hashtable$Entry@11310} "yarn.app.mapreduce.am.resource.mb" -> "3072"
  1001 = {Hashtable$Entry@11311} "mapreduce.framework.name" -> "yarn"
  1002 = {Hashtable$Entry@11312} "mapreduce.job.reduce.slowstart.completedmaps" -> "0.05"
  1003 = {Hashtable$Entry@11313} "yarn.resourcemanager.client.thread-count" -> "50"
  1004 = {Hashtable$Entry@11314} "mapreduce.cluster.temp.dir" -> "${hadoop.tmp.dir}/mapred/temp"
  1005 = {Hashtable$Entry@11315} "dfs.client.mmap.enabled" -> "true"
  1006 = {Hashtable$Entry@11316} "yarn.nodemanager.webapp.xfs-filter.xframe-options" -> "SAMEORIGIN"
  1007 = {Hashtable$Entry@11317} "mapreduce.jobhistory.intermediate-done-dir" -> "/mr-history/tmp"
  1008 = {Hashtable$Entry@11318} "fs.s3a.attempts.maximum" -> "20"
 overlay = {Properties@7870}  size = 11
 classLoader = {MutableURLClassLoader@7871}
jobConfCacheKey = "rdd_4_job_conf"
shouldCloneJobConf = true